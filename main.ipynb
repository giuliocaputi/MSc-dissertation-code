{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3d1a95",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "from pandas.api.types import is_integer_dtype\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "from itertools import combinations\n",
    "import ta\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from torch import tensor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pmdarima import auto_arima    \n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from itertools import product\n",
    "import multiprocessing as mp\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import Counter, defaultdict\n",
    "from math import ceil\n",
    "from numpy import sqrt, exp, log\n",
    "from scipy.special import ndtr\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0830d687",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9081b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "strike_price = 100\n",
    "premium_call = 4\n",
    "premium_put = 6\n",
    "stock_prices = np.linspace(50, 150, 500)\n",
    "\n",
    "pnl_call = np.maximum(stock_prices - strike_price, 0) - premium_call\n",
    "pnl_put = np.maximum(strike_price - stock_prices, 0) - premium_put\n",
    "pnl_straddle = pnl_call + pnl_put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743db64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This produces the plots in Figure 1\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "call_color = \"#1f77b4\"\n",
    "put_color = \"#ff7f0e\"\n",
    "straddle_color = \"#2ca02c\"\n",
    "strike_color = \"#d62728\"\n",
    "\n",
    "#Call option PnL\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "plt.plot(stock_prices, pnl_call, label=\"Call Option Payoff\", color=call_color, linewidth=2)\n",
    "plt.axhline(0, color='black', linewidth=1, linestyle='--')\n",
    "plt.axvline(strike_price, color=strike_color, linestyle='--', linewidth=2, label=\"Strike Price\")\n",
    "plt.title(\"Payoff of a Call Option\", fontsize=30, weight=\"bold\")\n",
    "plt.xlabel(\"Stock Price at Expiration\", fontsize=25)\n",
    "plt.ylabel(\"Profit\", fontsize=25)\n",
    "plt.tick_params(axis=\"x\", colors=\"black\", labelsize=20)\n",
    "plt.tick_params(axis=\"y\", colors=\"black\", labelsize=20)\n",
    "plt.legend(fontsize=15, frameon=True, edgecolor='black')\n",
    "plt.grid(True, linestyle=':', linewidth=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "#Put option PnL\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "plt.plot(stock_prices, pnl_put, label=\"Put Option Payoff\", color=put_color, linewidth=2)\n",
    "plt.axhline(0, color='black', linewidth=1, linestyle='--')\n",
    "plt.axvline(strike_price, color=strike_color, linestyle='--', linewidth=2, label=\"Strike Price\")\n",
    "plt.title(\"Payoff of a Put Option\", fontsize=30, weight=\"bold\")\n",
    "plt.xlabel(\"Stock Price at Expiration\", fontsize=25)\n",
    "plt.ylabel(\"Profit\", fontsize=25)\n",
    "plt.tick_params(axis=\"x\", colors=\"black\", labelsize=20)\n",
    "plt.tick_params(axis=\"y\", colors=\"black\", labelsize=20)\n",
    "plt.legend(fontsize=15, frameon=True, edgecolor='black')\n",
    "plt.grid(True, linestyle=':', linewidth=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "#Straddle PnL\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "plt.plot(stock_prices, pnl_straddle, label=\"Straddle Payoff\", color=straddle_color, linewidth=2)\n",
    "plt.axhline(0, color='black', linewidth=1, linestyle='--')\n",
    "plt.axvline(strike_price, color=strike_color, linestyle='--', linewidth=2, label=\"Strike Price\")\n",
    "plt.title(\"Payoff of a Straddle\", fontsize=30, weight=\"bold\")\n",
    "plt.xlabel(\"Stock Price at Expiration\", fontsize=25)\n",
    "plt.ylabel(\"Profit\", fontsize=25)\n",
    "plt.tick_params(axis=\"x\", colors=\"black\", labelsize=20)\n",
    "plt.tick_params(axis=\"y\", colors=\"black\", labelsize=20)\n",
    "plt.legend(fontsize=15, frameon=True, edgecolor='black')\n",
    "plt.grid(True, linestyle=':', linewidth=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810dfb57",
   "metadata": {},
   "source": [
    "# Data Collection and Preliminary Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will perform our analysis on the period 2013-2021 (9 years), \n",
    "#and backtest our trading strategy on the period 2022-2024 (3 years)\n",
    "\n",
    "start_date_analysis, end_date_analysis, end_date_backtest = \"2013-01-01\", \"2022-01-01\", \"2025-01-01\"\n",
    "tickers = [\"AAPL\", \"ABBV\", \"AMD\", \"AMGN\", \"AMZN\", \"AVGO\", \"AXP\", \"BAC\", \"BMY\", \"C\", \"GILD\", \"GOOG\", \"GS\", \"JNJ\", \"JPM\", \n",
    "           \"LLY\", \"MA\", \"META\", \"MRK\", \"MS\", \"MSFT\", \"NVDA\", \"ORCL\", \"PFE\", \"REGN\", \"SCHW\", \"TSLA\", \"V\", \"VRTX\", \"WFC\"]\n",
    "n_tickers = len(tickers)   #30 tickers in this study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a575338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_of = {}\n",
    "for tkr in tickers:\n",
    "    try:\n",
    "        sector_of[tkr] = yf.Ticker(tkr).info.get(\"sector\", \"Unknown\")\n",
    "    except Exception:\n",
    "        sector_of[tkr] = \"Unknown\"\n",
    "\n",
    "unique_sectors = sorted({s for s in sector_of.values() if s != \"Unknown\"})\n",
    "sector_dict = {s: i + 1 for i, s in enumerate(unique_sectors)}\n",
    "sector_dict[\"Unknown\"] = 0         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2314d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "for tkr in tickers:\n",
    "    try:\n",
    "        tmp = (\n",
    "            yf.Ticker(tkr)\n",
    "              .history(start=start_date_analysis, end=end_date_analysis, auto_adjust=False)\n",
    "              .reset_index()\n",
    "              .assign(\n",
    "                  Ticker=tkr,\n",
    "                  Sector=sector_of[tkr]\n",
    "              )\n",
    "              .loc[:, [\"Date\", \"Ticker\", \"Sector\",\n",
    "                       \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "        )\n",
    "        frames.append(tmp)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {tkr} (price fetch failed): {e}\")\n",
    "\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "#Convert Date to plain date\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"]).dt.date\n",
    "\n",
    "#Map \"Sector\" to its integer code (filling any unknown sectors with 0)\n",
    "df[\"Sector\"] = df[\"Sector\"].map(sector_dict).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of trading days per ticker\n",
    "n = round(len(df) / n_tickers)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ac0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No missing values\n",
    "len(df[df.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24344149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have a single entry with a \"Volume\" value of 0 (on that day, AMD moved its listing from NYSE to NASDAQ, so no\n",
    "#trading occurred for this stock)\n",
    "df[(df[[\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]] <= 0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To avoid issues later, we change this data to the data of the previous trading day\n",
    "for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]:\n",
    "    df.loc[5038, col] = df.loc[5037, col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505923db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The tickers are grouped and sorted as expected\n",
    "df[\"Ticker\"].tolist() == [stock for stock in tickers for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56498cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dates are in increasing order for all tickers\n",
    "check = True\n",
    "grouped_df = df.groupby(\"Ticker\")\n",
    "is_date_order_maintained = True\n",
    "for ticker, group in grouped_df:\n",
    "    if not group[\"Date\"].is_monotonic_increasing: check = False\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bff55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All tickers contain the same dates\n",
    "common_dates = None\n",
    "for ticker, group in grouped_df:\n",
    "    unique_dates = set(group[\"Date\"])\n",
    "    if common_dates is None:\n",
    "        common_dates = unique_dates\n",
    "    elif common_dates != unique_dates:\n",
    "        common_dates = common_dates.intersection(unique_dates)\n",
    "\n",
    "common_dates == set(df[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077dab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We check the correlations between pairs of stocks. To do so, we first create a table with 1 + \n",
    "#\"n_tickers\" columns (\"Date\" + \"n_tickers\" columns for the close prices of each stock). We name \n",
    "#these columns by the corresponding ticker\n",
    "\n",
    "df_by_ticker = df.pivot(index=\"Date\", columns=\"Ticker\", values=\"Close\")\n",
    "df_by_ticker.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deac0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates the plot in Figure 2\n",
    "\n",
    "stocks_corr_matrix = df_by_ticker.iloc[:, 1:].corr() * 100 \n",
    "    \n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "\n",
    "sns.heatmap(stocks_corr_matrix, annot=True, fmt='.0f', cmap=\"coolwarm\", cbar=True, \n",
    "annot_kws={\"color\": \"black\", \"fontsize\": 14}, linewidths=0.5, linecolor=\"gray\")\n",
    "    \n",
    "plt.title(\"Correlation Matrix of the Different Stocks\", color=\"black\", fontsize=20, weight=\"bold\")      \n",
    "plt.xlabel(\"Close Price of Different Stocks\", color=\"black\", fontsize=20)\n",
    "plt.ylabel(\"Close Price of Different Stocks\", color=\"black\", fontsize=20)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor(\"white\")\n",
    "ax.tick_params(axis=\"x\", colors=\"black\", labelrotation=45, labelsize=10)\n",
    "ax.tick_params(axis=\"y\", colors=\"black\", labelrotation=0, labelsize=10)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_edgecolor(\"black\")\n",
    "    spine.set_linewidth(1)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fea66e",
   "metadata": {},
   "source": [
    "# Feature and Target Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30224dc",
   "metadata": {},
   "source": [
    "### Feature Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57670f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we define some helper functions to aid in the computation of some predictors in the cell below\n",
    "\n",
    "x_vector = np.arange(20)\n",
    "x_vector_mean = x_vector.mean()\n",
    "x_vector_var = x_vector.var(ddof=0)\n",
    "\n",
    "def _slope_computer(y):\n",
    "    y_mean = y.mean()\n",
    "    cov_xy = ((x_vector - x_vector_mean) * (y - y_mean)).mean()\n",
    "    return (cov_xy / x_vector_var) / y.mean()  #% per day\n",
    "\n",
    "\n",
    "triu_mask = np.triu_indices(20, k=1)  #i<j mask for a 20×20 matrix\n",
    "\n",
    "def _tau_weighted(y):\n",
    "    \"\"\"\n",
    "    Modified Kendall's Tau statistic on a fixed 20-day window of prices.\n",
    "\n",
    "    C = Σ max(price_j - price_i, 0)      over all i<j\n",
    "    D = Σ max(price_i - price_j, 0)      over all i<j\n",
    "    τ = (C - D) / (C + D)                ∈ [-1, +1]\n",
    "    \"\"\"\n",
    "    #pair-wise differences for i<j\n",
    "    y = np.asarray(y)\n",
    "    diff = y[None, :] - y[:, None]           #diff[i,j] = price_j − price_i\n",
    "    diff = diff[triu_mask]                   #keep only i<j half (190 values)\n",
    "\n",
    "    C = diff[diff > 0].sum()             #concordant (up-moves),   ≥ 0\n",
    "    D = (-diff[diff < 0]).sum()          #discordant (down-moves), ≥ 0\n",
    "    denom = C + D\n",
    "    if denom == 0:\n",
    "        return np.nan                    #when all prices are identical, return NaN\n",
    "    return (C - D) / denom\n",
    "\n",
    "spy_return = yf.Ticker(\"SPY\").history(start=start_date_analysis, end=end_date_analysis, auto_adjust=False)[\"Close\"].pct_change()\n",
    "spy_return.index = spy_return.index.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d670450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function computes several predictors derived from price and volume data. They are all computed by-ticker\n",
    "\n",
    "def add_features(df_ticker):\n",
    "    \"\"\"\n",
    "    This function takes a DataFrame for a single ticker and adds several columns, all computed by-ticker\n",
    "    \"\"\"\n",
    "    df = df_ticker.copy()\n",
    "    prices = df[\"Close\"]\n",
    "\n",
    "    #Normalised slope of linear regression fitted with y = past prices and x = [0, 1, ..., 19]\n",
    "    df[\"Slope\"] = prices.rolling(20, min_periods=20).apply(_slope_computer)\n",
    "\n",
    "    #Magnitude-Aware Kendall’s Tau statistic, computed on past prices\n",
    "    df[\"Tau\"] = prices.rolling(20, min_periods=20).apply(_tau_weighted)\n",
    "\n",
    "    #Day-by-day return\n",
    "    ticker_return = prices.pct_change()\n",
    "    df[\"Return\"] = ticker_return\n",
    "\n",
    "    #Coefficient and slope of linear regression fitted with y = past prices and x = past SPY prices on the same dates\n",
    "    spy = (spy_return.reindex(df[\"Date\"]).reset_index(drop=True))\n",
    "    spy.index = df.index\n",
    "    cov_spy_and_ticker = spy.rolling(30, min_periods=30).cov(ticker_return)\n",
    "    var_spy = spy.rolling(30, min_periods=30).var()\n",
    "    beta = cov_spy_and_ticker / var_spy.replace(0, np.nan)     #avoid /0\n",
    "    alpha = ticker_return.rolling(30, min_periods=30).mean() - beta * spy.rolling(30, min_periods=30).mean()\n",
    "    df[\"Beta\"] = beta\n",
    "    df[\"Alpha\"] = alpha\n",
    "\n",
    "    #Approximate number of shares traded\n",
    "    df[\"Shares_traded\"] = df[\"Volume\"] / prices\n",
    "\n",
    "    #Exponentially weighted volatility (standard deviation of returns)\n",
    "    df[\"Volatility\"] = df[\"Return\"].ewm(span=20).std()\n",
    "    \n",
    "    #Normalised difference between exponential moving averages\n",
    "    ema_10 = ta.trend.EMAIndicator(close=df[\"Close\"], window=10, fillna=False).ema_indicator()\n",
    "    ema_30 = ta.trend.EMAIndicator(close=df[\"Close\"], window=30, fillna=False).ema_indicator()\n",
    "    df[\"EMA_diff\"] = (ema_10 - ema_30) / prices\n",
    "    \n",
    "    #Momentum (using MACD)\n",
    "    macd_indicator = ta.trend.MACD(\n",
    "        close=df[\"Close\"], window_slow=24, window_fast=12, window_sign=9, fillna=False)\n",
    "    hist_series = macd_indicator.macd_diff()\n",
    "    df[\"MACD_momentum\"] = np.where((hist_series > 0) & (hist_series.diff() > 0), 2,\n",
    "        np.where((hist_series > 0) & (hist_series.diff() <= 0), 1,\n",
    "        np.where((hist_series <= 0) & (hist_series.diff() <= 0), -2,\n",
    "        np.where((hist_series <= 0) & (hist_series.diff() > 0), -1,\n",
    "                    np.nan))))\n",
    "\n",
    "    #Relative strength index\n",
    "    df[\"RSI\"] = ta.momentum.RSIIndicator(close=df[\"Close\"], window=14, fillna=False).rsi()\n",
    "    \n",
    "    #Bollinger bands binary variables\n",
    "    bb_indicator = ta.volatility.BollingerBands(close=df[\"Close\"], window=20, window_dev=2, fillna=False)            \n",
    "    bb_upper = bb_indicator.bollinger_hband()\n",
    "    bb_lower = bb_indicator.bollinger_lband()\n",
    "    df[\"Bol_up\"] = (prices > bb_upper).astype(int)\n",
    "    df[\"Bol_down\"] = (prices < bb_lower).astype(int)\n",
    "    \n",
    "    #Stochastic oscillator signal\n",
    "    stoch_indicator = ta.momentum.StochasticOscillator(high=df[\"High\"], low=df[\"Low\"], \n",
    "                      close=df[\"Close\"], window=14, smooth_window=3, fillna=False)             \n",
    "    df[\"Stoch_osc_signal\"] = stoch_indicator.stoch() - stoch_indicator.stoch_signal()\n",
    "    \n",
    "    #Commodity channel index\n",
    "    df[\"CCI\"] = cci_indicator = ta.trend.CCIIndicator(\n",
    "        high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], window=20, fillna=False).cci()\n",
    "\n",
    "    #On-balance volume\n",
    "    df[\"OBV\"] = ta.volume.OnBalanceVolumeIndicator(close=df[\"Close\"], volume=df[\"Volume\"], \n",
    "             fillna=False).on_balance_volume() / df[\"Volume\"]       \n",
    "    \n",
    "    #Money flow index\n",
    "    df[\"MFI\"] = ta.volume.MFIIndicator(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], \n",
    "                volume=df[\"Volume\"], window=14, fillna=False).money_flow_index()\n",
    "    \n",
    "    #Parabolic Stock and Reverse (SAR)\n",
    "    df[\"PSAR\"] = ta.trend.PSARIndicator(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"]\n",
    "                 ).psar() / prices\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff164de5",
   "metadata": {},
   "source": [
    "### Target Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9239f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function computes, for each stock on each day, the target variable and several lagged versions of it, to use \n",
    "#as predictors. In this analysis, we only work with one predictor, but the framework allows onw to experiment with \n",
    "#many targets, each with a different horizon\n",
    "\n",
    "def add_targets(df_ticker, target_horizons=(1, 5, 10), future=True):\n",
    "    \"\"\"\n",
    "    For each horizon \"h\" in \"target_horizons\", add a column that is the maximum absolute % move\n",
    "    (up or down) in the next (if \"future\" is True) or previous (if \"future\" is False) \"h\" trading days. \n",
    "    This column is called \"Target<h>\" if \"future\" is True, and \"Lagged_target<h>\" if \"future\" is False.\n",
    "    \"\"\"\n",
    "    df = df_ticker.copy()\n",
    "    high, low, close = df[\"High\"], df[\"Low\"], df[\"Close\"]\n",
    "\n",
    "    for h in target_horizons:\n",
    "        if future: #this part is forward-looking (so it is used to compute the target variables)\n",
    "            highest = high.shift(-1).iloc[::-1].rolling(h, min_periods=h).max().iloc[::-1]\n",
    "            lowest = low.shift(-1).iloc[::-1].rolling(h, min_periods=h).min().iloc[::-1]\n",
    "\n",
    "            #largest move in the next \"h\" days (from tomorrow)\n",
    "            df[f\"Target{h}\"] = np.maximum((highest - close).abs() / close, (lowest  - close).abs() / close)\n",
    "\n",
    "        else: #this part is backward-looking (so it is used to compute the target variables)\n",
    "            highest = high.rolling(h, min_periods=h).max()\n",
    "            lowest = low.rolling(h, min_periods=h).min()\n",
    "\n",
    "            #largest move in the previous \"h\" days (including today). This could be defined in many other ways\n",
    "            df[f\"Lagged_target{h}\"] = np.maximum((close - highest).abs() / highest, (close - lowest).abs() / lowest)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e367ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we apply the above functions to every ticker separately, adding both the predictors and the target to \"df\".\n",
    "#We choose to have only one target variable, while we use 5 lagged versions of the target as predictors\n",
    "\n",
    "horizons = (1, 3, 5, 10, 20)\n",
    "max_horizon = max(horizons)\n",
    "target_horizon = 10  #We may experiment with different target variables\n",
    "\n",
    "df = (df.groupby(\"Ticker\", group_keys=False, sort=False).apply(add_features)\n",
    ".groupby(\"Ticker\", group_keys=False, sort=False).apply(add_targets, target_horizons=horizons, future=False)\n",
    ".groupby(\"Ticker\", group_keys=False, sort=False).apply(add_targets, target_horizons=(target_horizon,), future=True))\n",
    "\n",
    "df = df.rename(columns={f\"Target{target_horizon}\" : \"Target\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386ea2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each ticker, the first \"h-1\" values of \"Lagged_target<h>\" and the last \"h\" values of \"Target<h>\" are null\n",
    "df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23878243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comments on the \"Tau\" variable. This creates the plots in Figure 3\n",
    "\n",
    "for i in range(8):\n",
    "    ticker = tickers[i]\n",
    "    dfplot = df[df[\"Ticker\"] == ticker]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    \n",
    "    dates_plot = np.array(dfplot[\"Date\"])\n",
    "    prices_plot = np.array(dfplot[\"Close\"])\n",
    "    tau_plot = np.array(dfplot[\"Tau\"])\n",
    "    discrete_tau_plot = np.where(tau_plot >  0.4,  1, np.where(tau_plot < -0.4, -1, 0))\n",
    "\n",
    "    color_map = {1: \"green\", 0: \"blue\", -1: \"red\"} #This is the color mapping\n",
    "\n",
    "    #We iterate through the data and plot segments with different colours\n",
    "    for i in range(len(dates_plot) - 1):\n",
    "        plt.plot(dates_plot[i:i+2], prices_plot[i:i+2], color=color_map[discrete_tau_plot[i]])\n",
    "\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color=\"green\", lw=2, label=\"Price when Tau > 0.4\"),\n",
    "        Line2D([0], [0], color=\"blue\", lw=2, label=\"Price when -0.4 ≤ Tau ≤ 0.4\"),\n",
    "        Line2D([0], [0], color=\"red\", lw=2, label=\"Price when Tau < -0.4\")]\n",
    "\n",
    "    legend = plt.legend(handles=legend_elements, loc=\"upper left\", fontsize=17, frameon=True)\n",
    "    legend.get_frame().set_edgecolor(\"black\")  \n",
    "    legend.get_frame().set_linewidth(2)  \n",
    "\n",
    "    plt.xlabel(\"Date\", color=\"black\", fontsize=28)\n",
    "    plt.ylabel(\"Price\", color=\"black\", fontsize=28)\n",
    "    plt.title(f\"Close Price of {ticker} Over Time, Coloured by Tau\", color=\"black\", \n",
    "              fontsize=28, weight=\"bold\")          \n",
    "    plt.grid(color=\"gray\", linewidth=0.5)\n",
    "\n",
    "    ax.tick_params(axis=\"x\", colors=\"black\", labelsize=20)\n",
    "    ax.tick_params(axis=\"y\", colors=\"black\", labelsize=20)\n",
    "\n",
    "    ax.set_facecolor(\"white\")\n",
    "    ax.tick_params(axis=\"x\", colors=\"black\")\n",
    "    ax.tick_params(axis=\"y\", colors=\"black\")\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor(\"black\")\n",
    "        spine.set_linewidth(1)\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da95f79e",
   "metadata": {},
   "source": [
    "# Directional Mean Squared Error Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b444f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This shows that \"Target\" is a very good proxy of realised volatility, as \"Volatility\" has a 75%\n",
    "#correlation with (a 10-day shifted version of) \"Target\"\n",
    "\n",
    "df[\"Target_shifted_10d\"] = df.groupby(\"Ticker\")[\"Target\"].shift(10)\n",
    "df_clean = df.dropna(subset=[\"Target_shifted_10d\"])\n",
    "print(df_clean[\"Volatility\"].corr(df_clean[\"Target_shifted_10d\"]))\n",
    "df.drop(columns=[\"Target_shifted_10d\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187144e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of the Directional Mean Squared Error (DMSE) loss function\n",
    "\n",
    "loss_penalty = 9\n",
    "class DMSE(nn.Module):\n",
    "    def __init__(self, over_penalty = loss_penalty, reduction = \"mean\"):\n",
    "        super().__init__()\n",
    "        self.over_penalty = over_penalty\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, y_hat: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        diff2 = (y - y_hat) ** 2\n",
    "        loss = torch.where(y_hat <= y, diff2, diff2 * self.over_penalty)\n",
    "        return loss.mean() if self.reduction == \"mean\" else loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a7274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell creates Figure 4\n",
    "\n",
    "y = 1.0  #true value\n",
    "y_hat = np.linspace(0, 2, 400)  #predicted values\n",
    "loss = np.where(y_hat <= y, (y - y_hat)**2, loss_penalty * (y - y_hat)**2)  #DMSE\n",
    "\n",
    "#Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(y_hat, loss, label='DMSE Loss', color='blue')\n",
    "plt.axvline(x=y, color='gray', linestyle='--', label='True Value y = 1.0')\n",
    "plt.xlabel('Prediction $\\hat{y}$', fontsize=23)\n",
    "plt.ylabel('Loss $L(y, \\hat{y})$', fontsize=23)\n",
    "plt.title('Directional Mean Squared Error Loss Function', color=\"black\", fontsize=20, weight=\"bold\")\n",
    "plt.tick_params(axis=\"x\", colors=\"black\", labelsize=20)\n",
    "plt.tick_params(axis=\"y\", colors=\"black\", labelsize=20)\n",
    "\n",
    "legend = plt.legend(frameon=True, fontsize=16)\n",
    "legend.get_frame().set_edgecolor('black')\n",
    "legend.get_frame().set_linewidth(1.2)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35457646",
   "metadata": {},
   "source": [
    "# Pre-Modelling Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11bfcf1",
   "metadata": {},
   "source": [
    "### Slightly Reduced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd46ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We exclude from this part of the analysis every row containing null values, keeping the order of the data unchanged\n",
    "\n",
    "df2 = df.dropna().copy()\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd2710c",
   "metadata": {},
   "source": [
    "### Correlation Between Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f9fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We compute and plot the correlation matrix of the variables, which is shown in Figure 5\n",
    "\n",
    "corr_matrix = df2.drop([\"Date\", \"Ticker\"], axis=1).corr() * 100\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.0f',cmap='coolwarm', cbar=True, \n",
    "            annot_kws={\"color\": \"black\", \"fontsize\": 14}, linewidths=0.5, linecolor=\"gray\")\n",
    "\n",
    "plt.title(\"Correlation Matrix of our Variables\", color=\"black\", fontsize=30, weight=\"bold\")\n",
    "plt.xlabel(\" \", color=\"black\", fontsize=20)\n",
    "plt.ylabel(\" \", color=\"black\", fontsize=20)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor(\"white\")\n",
    "ax.tick_params(axis=\"x\", colors=\"black\", labelrotation=90, labelsize=15)\n",
    "ax.tick_params(axis=\"y\", colors=\"black\", labelsize=15)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_edgecolor(\"black\")\n",
    "    spine.set_linewidth(1)     \n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae44c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop([\"High\", \"Low\", \"Open\", \"Close\", \"Volume\", \"Slope\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81587fbe",
   "metadata": {},
   "source": [
    "### Mixed-Effects Models for Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f4dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_predictors = ['Tau', 'Return', 'Beta', 'Alpha', 'Shares_traded', 'Volatility', 'EMA_diff', 'RSI', 'Stoch_osc_signal', \n",
    "'CCI', 'OBV', 'MFI', 'PSAR', 'Lagged_target1', 'Lagged_target3', 'Lagged_target5', 'Lagged_target10', 'Lagged_target20']\n",
    "categorical_predictors = ['Sector', 'MACD_momentum', 'Bol_up', 'Bol_down']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to label the significance of p-values\n",
    "def significance_label(p):\n",
    "    if pd.isna(p): return \"\"\n",
    "    elif p <= 0.001: return \"***\"\n",
    "    elif p <= 0.01: return \"**\"\n",
    "    elif p <= 0.05: return \"*\"\n",
    "    else: return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61112bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fit a mixed linear model for each continuous predictor. This cell creates Table 1\n",
    "\n",
    "pvalue_column = []\n",
    "group_var_column = []\n",
    "for var in continuous_predictors:\n",
    "    formula = f\"Target ~ {var}\"\n",
    "    model = smf.mixedlm(formula, df2, groups=df2[\"Ticker\"])\n",
    "    result = model.fit(reml=False, method='lbfgs', maxiter=1000)\n",
    "    pvalue_column.append(result.pvalues[var])\n",
    "    group_var_column.append(float(result.cov_re.iloc[0, 0]))\n",
    "    #print(result.summary())\n",
    "        \n",
    "data = pd.DataFrame({\"Variable\" : continuous_predictors, \"Group Variance\" : group_var_column, \"P-value\" : pvalue_column})\n",
    "data[\"Significance\"] = data[\"P-value\"].apply(significance_label)\n",
    "data.sort_values(by=\"P-value\", inplace=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b90dd9",
   "metadata": {},
   "source": [
    "### Visual Exploration of the Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b898bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Density plot of \"Target\". This creates the plot in Figure 6\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "sns.kdeplot(data=df2[\"Target\"], fill=True, color='turquoise', linewidth=2)\n",
    "\n",
    "plt.title(f\"Density of Target\", fontsize=28, weight=\"bold\")\n",
    "plt.ylabel(\"Density\", fontsize=24)\n",
    "plt.xlabel(\"Target\", fontsize=24)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor(\"white\")\n",
    "ax.tick_params(axis=\"y\", colors=\"black\", labelsize=19)\n",
    "ax.tick_params(axis=\"x\", colors=\"black\", labelsize=19)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_edgecolor(\"black\")\n",
    "    spine.set_linewidth(1)\n",
    "\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of \"Target\" conditional on the quantiles of each continuous predictor.\n",
    "#This creates the first part of Figure 7\n",
    "\n",
    "quantile_bins = [0.2, 0.4, 0.6, 0.8]\n",
    "palette_red1 = [\"lightcoral\", \"salmon\", \"coral\", \"tomato\", \"orangered\"]\n",
    "\n",
    "for var in continuous_predictors:\n",
    "    #Compute quantiles for the variable\n",
    "    quantiles = df2[var].quantile(quantile_bins).values\n",
    "    bins = [-np.inf] + list(quantiles) + [np.inf]\n",
    "    labels = [f\"<Q20\", \"[Q20,Q40)\", \"[Q40,Q60)\", \"[Q60,Q80)\", \"≥Q80\"]\n",
    "\n",
    "    #Bin the variable\n",
    "    df2[f'{var}_bin'] = pd.cut(df2[var], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    #Prepare data: list of Target values grouped by binned variable\n",
    "    data = [df2.loc[df2[f'{var}_bin'] == lab, 'Target'].dropna() for lab in labels]\n",
    "\n",
    "    #Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bp = plt.boxplot(\n",
    "        data,\n",
    "        labels=labels,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor='lightblue', color='black'),\n",
    "        medianprops=dict(color='black'),\n",
    "        whiskerprops=dict(color='black'),\n",
    "        capprops=dict(color='black'),\n",
    "        flierprops=dict(marker='o', color='black', markersize=5)\n",
    "    )\n",
    "\n",
    "    for box, col in zip(bp['boxes'], palette_red1):\n",
    "        box.set_facecolor(col)\n",
    "\n",
    "    plt.title(f\"Target by {var} quantiles\", fontsize=25, weight=\"bold\")\n",
    "    plt.xlabel(f\"{var} quantile bin\", fontsize=30)\n",
    "    plt.ylabel(\"Target\", fontsize=30)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor(\"white\")\n",
    "    ax.tick_params(axis=\"y\", colors=\"black\", labelsize=21)\n",
    "    ax.tick_params(axis=\"x\", colors=\"black\", labelsize=20)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor(\"black\")\n",
    "        spine.set_linewidth(1)\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    df2.drop([f'{var}_bin'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbfc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of \"Target\" conditional on the categories of each categorical predictor.\n",
    "#This creates the second part of Figure 7\n",
    "\n",
    "palette_red2 = [\"mistyrose\", \"lightcoral\", \"salmon\", \"darksalmon\", \"coral\", \"tomato\", \n",
    "                \"orangered\", \"orange\", \"darkorange\", \"red\", \"firebrick\", \"darkred\"]\n",
    "\n",
    "for var in categorical_predictors:\n",
    "    categories = sorted(df2[var].dropna().unique())\n",
    "    data = [df2.loc[df2[var] == cat, 'Target'].dropna() for cat in categories]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bp = plt.boxplot(data, labels=categories, patch_artist=True, boxprops=dict(facecolor='lightblue', color='black'),\n",
    "        medianprops=dict(color='black'), whiskerprops=dict(color='black'), capprops=dict(color='black'), \n",
    "        flierprops=dict(marker='o', color='black', markersize=5))\n",
    "\n",
    "    #Colour the boxes\n",
    "    for idx, box in enumerate(bp['boxes']):\n",
    "        box.set_facecolor(palette_red2[idx % len(palette_red2)])\n",
    "\n",
    "    plt.title(f\"Target by {var} categories\", fontsize=25, weight=\"bold\")\n",
    "    plt.xlabel(f\"{var}\", fontsize=30)\n",
    "    plt.ylabel(\"Target\", fontsize=30)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor(\"white\")\n",
    "    ax.tick_params(axis=\"y\", colors=\"black\", labelsize=21)\n",
    "    ax.tick_params(axis=\"x\", colors=\"black\", labelsize=20)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor(\"black\")\n",
    "        spine.set_linewidth(1)\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0cb41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of each continuous predictor conditional on \"Target\". \n",
    "#This creates the first part of Figure 8\n",
    "\n",
    "bins   = [-np.inf, 0.04, 0.08, 0.12, 0.20, np.inf]\n",
    "labels = ['≤0.04', '(0.04,0.08]', '(0.08,0.12]', '(0.12,0.20]', '>0.20']\n",
    "\n",
    "df2['Target_bin'] = pd.cut(df2['Target'], bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "\n",
    "palette_blue = [\"deepskyblue\", \"dodgerblue\", \"cornflowerblue\", \"royalblue\", \"blue\"]\n",
    "\n",
    "for var in continuous_predictors: \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    data = [df2.loc[df2['Target_bin'] == lab, var].dropna() for lab in labels]\n",
    "\n",
    "    bp = plt.boxplot(\n",
    "        data,\n",
    "        labels=labels,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor='lightblue', color='black'),\n",
    "        medianprops=dict(color='black'),\n",
    "        whiskerprops=dict(color='black'),\n",
    "        capprops=dict(color='black'),\n",
    "        flierprops=dict(marker='o', color='black', markersize=5))\n",
    "    \n",
    "    \n",
    "    for box, col in zip(bp['boxes'], palette_blue):\n",
    "        box.set_facecolor(col)\n",
    "\n",
    "    plt.title(f\"{var} by Target\", fontsize=25, weight=\"bold\")\n",
    "    plt.xlabel(\"Target\", fontsize=30)\n",
    "    plt.ylabel(var, fontsize=30)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor(\"white\")\n",
    "    ax.tick_params(axis=\"y\", colors=\"black\", labelsize=21)\n",
    "    ax.tick_params(axis=\"x\", colors=\"black\", labelsize=18)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor(\"black\")\n",
    "        spine.set_linewidth(1)\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0fcad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of each categorical predictor conditional on \"Target\". \n",
    "#This creates the second part of Figure 8\n",
    "\n",
    "for var in categorical_predictors:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    count_tbl = df2.groupby(['Target_bin', var], observed=False).size().reset_index(name='count')\n",
    "    pivot_cnt = (count_tbl\n",
    "                 .pivot(index=var, columns='Target_bin', values='count')\n",
    "                 .fillna(0))\n",
    "    pivot_freq = pivot_cnt.div(pivot_cnt.sum(axis=1), axis=0) \n",
    "\n",
    "    ax = pivot_freq.plot(kind='bar', width=0.8, color=palette_blue[:pivot_freq.shape[1]], figsize=(10, 6))\n",
    "\n",
    "    plt.title(f\"{var} by Target\", fontsize=25, weight=\"bold\")\n",
    "    plt.xlabel(var, fontsize=30)\n",
    "    plt.ylabel(\"Frequency\", fontsize=30)\n",
    "\n",
    "    ax.set_facecolor(\"white\")\n",
    "    ax.tick_params(axis=\"y\", colors=\"black\", labelsize=21)\n",
    "    ax.tick_params(axis=\"x\", colors=\"black\", labelsize=18)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)  \n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor(\"black\")\n",
    "        spine.set_linewidth(1)\n",
    "\n",
    "    legend = plt.legend(title=\"Target\",\n",
    "                        loc='upper right',  \n",
    "                        frameon=True, fontsize=18)\n",
    "    legend.get_title().set_fontsize(18)\n",
    "    legend.get_frame().set_edgecolor(\"black\")\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop([\"Target_bin\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f77a74",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e48686",
   "metadata": {},
   "source": [
    "### Preliminary Steps for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd22c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encode the \"Sector\" variable\n",
    "sector_dummies = pd.get_dummies(df2[\"Sector\"], prefix=\"Sector\", dtype=int, drop_first=True)\n",
    "df2 = pd.concat([df2, sector_dummies], axis=1)\n",
    "\n",
    "#This list contains the names of all the candidate predictors\n",
    "predictors = continuous_predictors + categorical_predictors + list(sector_dummies.columns)\n",
    "predictors.remove(\"Sector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b9706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train–test split (first 7 years vs. following 2 years) + per-ticker scaling\n",
    "\n",
    "df_train = df2[pd.to_datetime(df2[\"Date\"]) < pd.to_datetime(\"2020-01-01\")]\n",
    "df_test = df2[pd.to_datetime(df2[\"Date\"]) >= pd.to_datetime(\"2020-01-01\")]\n",
    "\n",
    "#Per-ticker mean and std for every predictor, computed on the train period\n",
    "scaler_dict_analysis = {} #This will be a nested dictionary indexed by ticker. For every ticker, it will contain the mean and std of each predictor on the train set    \n",
    "for ticker, g in df_train.groupby(\"Ticker\"): #\"g\" is a sub-dataframe for one ticker\n",
    "    mu = g[continuous_predictors].mean().astype(\"float32\") #Pandas Series indexed by the predictors to standardise. The values are the predictors' means\n",
    "    std = g[continuous_predictors].std(ddof=0).replace(0, 1e-12).astype(\"float32\")  #avoid /0\n",
    "    scaler_dict_analysis[ticker] = {\"mean\": mu, \"std\": std}\n",
    "\n",
    "\n",
    "#helper function to standardise the continuous predictors by ticker using the means and stds stored in \"scaler_dict_analysis\"\n",
    "def standardise(df_part: pd.DataFrame, variables: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a copy of df_part where the columns in *variables* are\n",
    "    standardised (per-ticker) using the mean / std computed on the train set.\n",
    "    All other columns are left unchanged.\n",
    "    \"\"\"\n",
    "    pieces = []\n",
    "    for ticker, g in df_part.groupby(\"Ticker\", sort=False):\n",
    "        m = scaler_dict_analysis[ticker][\"mean\"]\n",
    "        s = scaler_dict_analysis[ticker][\"std\"]\n",
    "        g_std = g.copy()\n",
    "        g_std[variables] = ((g_std[variables] - m) / s).astype(\"float32\")\n",
    "        pieces.append(g_std)\n",
    "\n",
    "    #Re-assemble in original row order\n",
    "    df_out = pd.concat(pieces).loc[df_part.index]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ea1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_stand = standardise(df_train, continuous_predictors)\n",
    "df_test_stand = standardise(df_test, continuous_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b047c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stand, y_train = df_train_stand[predictors], df_train[\"Target\"]\n",
    "X_test_stand, y_test = df_test_stand[predictors], df_test[\"Target\"]\n",
    "\n",
    "#We prepare the data\n",
    "X_train_stand_np = X_train_stand.values.astype(\"float32\")\n",
    "X_test_stand_np = X_test_stand.values.astype(\"float32\")\n",
    "y_train_np = y_train.values.astype(\"float32\").reshape(-1, 1)\n",
    "y_test_np = y_test.values.astype(\"float32\").reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c866fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function trains a model by minimising the DMSE on the given training set\n",
    "\n",
    "def train_model(model_type, X_train, y_train, *, epochs=70, learn_rate=1e-2, \n",
    "                over_penalty=loss_penalty, batch_size=1024, seed=0):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #for running-time purposes\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    Xtr = torch.as_tensor(X_train, dtype=torch.float32, device=device)\n",
    "    ytr = torch.as_tensor(y_train, dtype=torch.float32, device=device)\n",
    "\n",
    "    train_dl = DataLoader(\n",
    "        TensorDataset(Xtr, ytr),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, \n",
    "        pin_memory=False,\n",
    "        num_workers=0,\n",
    "        persistent_workers=False)\n",
    "\n",
    "    if model_type == \"linear regression\":\n",
    "        model = nn.Linear(Xtr.shape[1], 1, bias=True)\n",
    "    elif model_type == \"neural network\":\n",
    "        #This builds an arbitrarily deep MLP from a list of widths\n",
    "        #Each element of \"hidden_sizes\" represents the width of a particular layer of the network\n",
    "        hidden_sizes = [1024, 512, 256, 256, 128, 128, 64, 32]\n",
    "        layers, in_dim = [], Xtr.shape[1]\n",
    "        for h in hidden_sizes:\n",
    "            layers.extend([nn.Linear(in_dim, h), nn.ReLU()])  #we use the relu activation function\n",
    "            in_dim = h\n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        model = nn.Sequential(*layers)\n",
    "\n",
    "    model = model.to(device) \n",
    "    criterion = DMSE(over_penalty=over_penalty) #This is the custom loss function defined above\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            optim.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c8e20",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3568bd04",
   "metadata": {},
   "source": [
    "##### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adf558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type-II ANOVA table. This creates the left-hand part of Table 2.\n",
    "\n",
    "formula = \"Target ~ \" + \" + \".join(predictors)  #a constant term is added by default\n",
    "lm = smf.ols(formula, data=df_train_stand).fit()\n",
    "anova_df_type2 = anova_lm(lm, typ=2).sort_values(\"PR(>F)\", na_position='last')\n",
    "anova_df_type2[\"Significance\"] = anova_df_type2[\"PR(>F)\"].apply(significance_label)\n",
    "anova_df_type2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the significance order given by the type-II ANOVA, we fit the usual Type-I (sequential) ANOVA.\n",
    "#This creates the right-hand part of Table 2.\n",
    "\n",
    "order = [p for p in anova_df_type2.index if p in predictors]  #this excludes \"Residual\"\n",
    "formula_ord = \"Target ~ 1 + \" + \" + \".join(order)\n",
    "\n",
    "lm_ord = smf.ols(formula_ord, data=df_train_stand).fit()\n",
    "\n",
    "anova_df_type1 = anova_lm(lm_ord, typ=1)\n",
    "anova_df_type1[\"Significance\"] = anova_df_type1[\"PR(>F)\"].apply(significance_label)\n",
    "anova_df_type1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555864a3",
   "metadata": {},
   "source": [
    "##### AIC and BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce14fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we define some helper functions to compute the actual loss, our estimate of sigma, \n",
    "#the log-likelihood, the AIC and the BIC under the DMSE framework.\n",
    "\n",
    "def dmse_piecewise_loss(residuals: np.ndarray, alpha: float) -> np.ndarray:\n",
    "    r2 = residuals**2\n",
    "    return np.where(residuals >= 0.0, r2, alpha * r2)\n",
    "\n",
    "\n",
    "def dmse_sigma_hat(residuals: np.ndarray, alpha: float) -> float:\n",
    "    return dmse_piecewise_loss(residuals, alpha).mean() \n",
    "\n",
    "\n",
    "def dmse_loglik(residuals: np.ndarray, alpha: float) -> float:\n",
    "    \"\"\"\n",
    "    Log-likelihood at (β̂, σ̂) under the two-piece Gaussian implied by DMSE.\n",
    "    Uses σ̂² = mean L_(alpha)(r) and log-lik = n(0.5 log(2/π) - 0.5 log σ̂² - log(1 + alpha^{-0.5})) - n/2.\n",
    "    \"\"\"\n",
    "    n = residuals.size\n",
    "    L = dmse_piecewise_loss(residuals, alpha)\n",
    "    sig2_hat = L.mean()                       #σ̂²\n",
    "    return n * (0.5*np.log(2/np.pi) - 0.5*np.log(sig2_hat) - np.log1p(1/np.sqrt(alpha))) - n/2\n",
    "\n",
    "\n",
    "def dmse_aic_bic(residuals: np.ndarray, alpha: float, n_params: int):\n",
    "    n = residuals.shape[0]\n",
    "    ll = dmse_loglik(residuals, alpha)\n",
    "    aic = -2*ll + 2*n_params\n",
    "    bic = -2*ll + n_params * np.log(n)\n",
    "    return ll, aic, bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde20eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IRLS for DMSE linear regression\n",
    "def fit_dmse_linear_irls(X: np.ndarray, y: np.ndarray, *, alpha=9, max_iter=20, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Returns (y_hat, residuals, coef, intercept).\n",
    "    X: (n, p), y: (n,)\n",
    "    We include an intercept by augmenting X with a column of 1s.\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    X1 = np.c_[np.ones((n, 1), dtype=np.float64), X.astype(np.float64, copy=False)]\n",
    "    y  = y.astype(np.float64, copy=False)\n",
    "\n",
    "    #start from OLS (cheap, good init)\n",
    "    beta = np.linalg.lstsq(X1, y, rcond=None)[0]  #(p+1,)\n",
    "\n",
    "    #IRLS loop\n",
    "    for _ in range(max_iter):\n",
    "        r = y - X1 @ beta\n",
    "        w = np.where(r >= 0.0, 1.0, float(alpha))      #DMSE weights\n",
    "        #WLS normal equations: (X^T W X) beta = X^T W y\n",
    "        WX  = X1 * w[:, None]\n",
    "        XT_W_X = X1.T @ WX\n",
    "        XT_W_y = X1.T @ (w * y)\n",
    "        beta_new = np.linalg.solve(XT_W_X, XT_W_y)\n",
    "        if np.max(np.abs(beta_new - beta)) <= tol:\n",
    "            beta = beta_new\n",
    "            break\n",
    "        beta = beta_new\n",
    "\n",
    "    y_hat = X1 @ beta\n",
    "    residuals = y - y_hat\n",
    "    intercept = beta[0]\n",
    "    coef = beta[1:]\n",
    "    return y_hat, residuals, coef, intercept\n",
    "\n",
    "#Scorer for a given subset\n",
    "class DMSESubsetScorer:\n",
    "    def __init__(self, X_df, y_series, alpha=9):\n",
    "        self.X_df = X_df\n",
    "        self.y = y_series.to_numpy(dtype=np.float64)\n",
    "        self.alpha = alpha\n",
    "        self.cache = {} \n",
    "\n",
    "    def score(self, feature_list):\n",
    "        key = frozenset(feature_list)\n",
    "        if key in self.cache:\n",
    "            return self.cache[key]\n",
    "\n",
    "        X = self.X_df[list(feature_list)].to_numpy(dtype=np.float64)\n",
    "        _, residuals, coef, intercept = fit_dmse_linear_irls(X, self.y, alpha=self.alpha)\n",
    "\n",
    "        #params = p betas + intercept + sigma\n",
    "        n_params = len(feature_list) + 2\n",
    "        ll, aic, bic = dmse_aic_bic(residuals, alpha=self.alpha, n_params=n_params)\n",
    "        payload = {\n",
    "            \"ll\": ll, \"aic\": aic, \"bic\": bic,\n",
    "            \"features\": list(feature_list),\n",
    "            \"coef\": coef, \"intercept\": intercept\n",
    "        }\n",
    "        self.cache[key] = payload\n",
    "        return payload\n",
    "\n",
    "#Stepwise search\n",
    "def stepwise_selection_dmse(\n",
    "    X_df, y_series, *, criterion=\"aic\", alpha=9, tol=1e-8, forward_only=False):\n",
    "    \"\"\"\n",
    "    - criterion: 'aic' or 'bic'\n",
    "    - forward_only: if True, skip backward sweeps (much faster)\n",
    "    \"\"\"\n",
    "    scorer = DMSESubsetScorer(X_df, y_series, alpha=alpha)\n",
    "    remaining = list(X_df.columns)\n",
    "    selected = []\n",
    "    current_score = np.inf\n",
    "    best_payload = None\n",
    "    rng = np.random.default_rng(0)\n",
    "\n",
    "    while remaining:\n",
    "        #Optionally sub-sample candidates to cut work\n",
    "        cand_pool = remaining\n",
    "\n",
    "        #forward step\n",
    "        candidates = []\n",
    "        for cand in cand_pool:\n",
    "            trial = selected + [cand]\n",
    "            payload = scorer.score(trial)\n",
    "            candidates.append((payload[criterion], cand, payload))\n",
    "        best_new_score, best_cand, best_payload = min(candidates, key=lambda t: t[0])\n",
    "\n",
    "        if best_new_score < current_score - tol:\n",
    "            remaining.remove(best_cand)\n",
    "            selected.append(best_cand)\n",
    "            current_score = best_new_score\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        if forward_only:\n",
    "            continue  #skip backward sweep\n",
    "\n",
    "        #backward sweep\n",
    "        improved = True\n",
    "        while improved and len(selected) > 1:\n",
    "            improved = False\n",
    "            drops = []\n",
    "            for combo in combinations(selected, len(selected) - 1):\n",
    "                payload = scorer.score(list(combo))\n",
    "                drops.append((payload[criterion], list(combo), payload))\n",
    "            best_drop_score, best_combo, best_drop_payload = min(drops, key=lambda t: t[0])\n",
    "            if best_drop_score < current_score - tol:\n",
    "                selected = best_combo\n",
    "                best_payload = best_drop_payload\n",
    "                current_score = best_drop_score\n",
    "                improved = True\n",
    "\n",
    "    #fallback to best singleton if none selected\n",
    "    if not selected:\n",
    "        singleton_payloads = [DMSESubsetScorer(X_df, y_series, alpha=alpha).score([c]) for c in X_df.columns]\n",
    "        best_payload = min(singleton_payloads, key=lambda d: d[criterion])\n",
    "\n",
    "    return best_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98faa9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = df_train_stand[predictors]\n",
    "y_all = df_train_stand[\"Target\"]\n",
    "ALPHA = loss_penalty\n",
    "\n",
    "best_aic = stepwise_selection_dmse(\n",
    "    X_all, y_all, criterion=\"aic\", alpha=ALPHA,\n",
    "    forward_only=False)\n",
    "\n",
    "best_bic = stepwise_selection_dmse(\n",
    "    X_all, y_all, criterion=\"bic\", alpha=ALPHA,\n",
    "    forward_only=False)\n",
    "\n",
    "print(\"\\nAIC-minimising model\")\n",
    "print(f\"Number of predictors: {len(best_aic['features'])}\")\n",
    "print(f\"AIC: {best_aic['aic']:.4f}\")\n",
    "print(f\"Log-likelihood: {best_aic['ll']:.4f}\")\n",
    "print(\"Selected features:\", best_aic[\"features\"])\n",
    "\n",
    "print(\"\\nBIC-minimising model\")\n",
    "print(f\"Number of predictors: {len(best_bic['features'])}\")\n",
    "print(f\"BIC: {best_bic['bic']:.4f}\")\n",
    "print(f\"Log-likelihood: {best_bic['ll']:.4f}\")\n",
    "print(\"Selected features:\", best_bic[\"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4164319",
   "metadata": {},
   "source": [
    "##### Out-of-Sample DMSE Estimation for Best Linear Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553270f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_lr = best_bic[\"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d5cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We train (on the train set) a single linear regression model on all the stocks. We then test it performance on the test set.\n",
    "#We only use the predictors selected by the stepwise procedure above.\n",
    "\n",
    "X_train_lr_subset = X_train_stand[predictors_lr].values.astype(\"float32\")\n",
    "X_test_lr_subset = X_test_stand[predictors_lr].values.astype(\"float32\")\n",
    "\n",
    "y_train_col = y_train_np if getattr(y_train_np, \"ndim\", 1) == 2 else y_train_np.reshape(-1, 1)\n",
    "\n",
    "final_lr_model = train_model(\n",
    "        \"linear regression\",\n",
    "        X_train_lr_subset,\n",
    "        y_train_col,\n",
    "        epochs=15,\n",
    "        learn_rate=5e-2,\n",
    "        batch_size=len(X_train_lr_subset)\n",
    "    )\n",
    "\n",
    "with torch.no_grad():\n",
    "    device = next(final_lr_model.parameters()).device\n",
    "    X_test_t = torch.as_tensor(X_test_lr_subset, dtype=torch.float32, device=device)\n",
    "    y_test_col = y_test_np if getattr(y_test_np, \"ndim\", 1) == 2 else y_test_np.reshape(-1, 1)\n",
    "    y_test_t = torch.as_tensor(y_test_col, dtype=torch.float32, device=device)\n",
    "    preds = final_lr_model(X_test_t)\n",
    "    preds = torch.clamp(preds, min=0.0)   #force negative predictions to 0\n",
    "    final_lr_dmse = DMSE(over_penalty=loss_penalty)(preds, y_test_t).item()\n",
    "\n",
    "print(f\"The global linear regression model achieves a DMSE of {final_lr_dmse:.5f} on the test set\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now plot, for the first 10 stocks, the actual and predicted \"Target\" over time, using the single linear \n",
    "#regression model trained above. This creates the images in Figure 9\n",
    "\n",
    "plot_df = df_test.loc[:, [\"Ticker\", \"Date\"]].copy()\n",
    "y_true = y_test_t.squeeze(1).cpu().numpy()\n",
    "y_pred = preds.squeeze(1).cpu().numpy()\n",
    "plot_df[\"TrueTarget\"] = y_true\n",
    "plot_df[\"PredTarget\"] = y_pred\n",
    "\n",
    "for tkr in tickers[:10]:\n",
    "    sub = plot_df.loc[plot_df[\"Ticker\"] == tkr].sort_values(\"Date\")\n",
    "    if sub.empty:\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(sub[\"Date\"], sub[\"TrueTarget\"], lw=2.0, label=\"True Target\")\n",
    "    ax.plot(sub[\"Date\"], sub[\"PredTarget\"], lw=2.0, label=\"Predicted Target\")\n",
    "\n",
    "    ax.set_title(f\"{tkr}: Actual and Predicted Target, Linear Regression Trained on all Stocks\", fontsize=20, weight=\"bold\")\n",
    "    ax.set_ylabel(\"Target\", fontsize=25)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.tick_params(axis=\"x\", colors=\"black\", labelsize=15)\n",
    "    ax.tick_params(axis=\"y\", colors=\"black\", labelsize=18)\n",
    "    ax.legend(loc=\"upper right\", frameon=True, fontsize=20)\n",
    "    ax.set_facecolor(\"white\")    \n",
    "    ax.grid(True, which=\"major\", linestyle=\"-\", alpha=0.15)\n",
    "\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811d5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now train a different linear regression model for each stock (only the first 10 here). The training is performed\n",
    "#on the first 7 years, the testing on the subsequent 2 years, and the predictors used are the same as before.\n",
    "\n",
    "single_lr_models = {}\n",
    "N_test = len(df_test)\n",
    "#container for the assembled per-ticker predictions in test-row order\n",
    "y_pred_single_all = np.full(N_test, np.nan, dtype=np.float32)\n",
    "\n",
    "\n",
    "for tkr in tickers[:10]:\n",
    "    idx_tr = np.flatnonzero((df_train[\"Ticker\"].values == tkr))\n",
    "    idx_te = np.flatnonzero((df_test[\"Ticker\"].values == tkr))\n",
    "\n",
    "    if len(idx_tr) == 0 or len(idx_te) == 0:\n",
    "        continue \n",
    "\n",
    "    #Slice the (already standardized) features + target for this ticker\n",
    "    X_tr_tkr = X_train_lr_subset[idx_tr, :]\n",
    "    y_tr_tkr = y_train_np[idx_tr]\n",
    "    y_tr_tkr = y_tr_tkr if getattr(y_tr_tkr, \"ndim\", 1) == 2 else y_tr_tkr.reshape(-1, 1)\n",
    "\n",
    "    #Train linear regression exactly as before\n",
    "    model_tkr = train_model(\n",
    "        \"linear regression\",\n",
    "        X_tr_tkr.astype(\"float32\"),\n",
    "        y_tr_tkr.astype(\"float32\"),\n",
    "        epochs=15,\n",
    "        learn_rate=5e-2,\n",
    "        batch_size=len(X_tr_tkr),\n",
    "        seed=0\n",
    "    )\n",
    "    single_lr_models[tkr] = model_tkr\n",
    "\n",
    "    #Predict on that ticker's test rows and clamp negatives to 0\n",
    "    X_te_tkr = X_test_lr_subset[idx_te, :].astype(\"float32\")\n",
    "    with torch.no_grad():\n",
    "        device_tkr = next(model_tkr.parameters()).device\n",
    "        X_te_t = torch.as_tensor(X_te_tkr, dtype=torch.float32, device=device_tkr)\n",
    "        preds_t = model_tkr(X_te_t)\n",
    "        preds_t = torch.clamp(preds_t, min=0.0).squeeze(1).cpu().numpy().astype(np.float32)\n",
    "\n",
    "    #Insert into the right positions of the full test array\n",
    "    y_pred_single_all[idx_te] = preds_t\n",
    "\n",
    "mask = ~np.isnan(y_pred_single_all)\n",
    "with torch.no_grad():\n",
    "    device_eval = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    y_true_masked = (y_test_np[mask] if getattr(y_test_np, \"ndim\", 1) == 1 else y_test_np[mask, 0]).reshape(-1, 1)\n",
    "    y_pred_masked = y_pred_single_all[mask].reshape(-1, 1)\n",
    "\n",
    "    y_true_t = torch.as_tensor(y_true_masked, dtype=torch.float32, device=device_eval)\n",
    "    y_pred_t = torch.as_tensor(y_pred_masked, dtype=torch.float32, device=device_eval)\n",
    "    dmse_single_lr = DMSE(over_penalty=loss_penalty)(y_pred_t, y_true_t).item()\n",
    "\n",
    "print(f\"(Per-ticker) linear regression achieves a DMSE of {dmse_single_lr:.5f} on the test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11efdd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now plot, for the first 10 stocks, the actual and predicted \"Target\" over time (on the test set), using the per-ticker\n",
    "#linear regression models trained above. The predictors used are the same as above. This creates the images in Figure 10\n",
    "\n",
    "plot_df_single = df_test.loc[:, [\"Ticker\", \"Date\"]].copy()\n",
    "\n",
    "if \"y_true_1d\" not in globals():\n",
    "    y_true_1d = (y_test_np.astype(\"float32\").reshape(-1)\n",
    "                 if getattr(y_test_np, \"ndim\", 1) == 1\n",
    "                 else y_test_np[:, 0].astype(\"float32\"))\n",
    "\n",
    "#Per-ticker predictions assembled in test-row order\n",
    "plot_df_single[\"TrueTarget\"]  = y_true_1d\n",
    "plot_df_single[\"PredTarget\"]  = y_pred_single_all.astype(\"float32\")\n",
    "\n",
    "for tkr in tickers[:10]:\n",
    "    sub = plot_df_single.loc[plot_df_single[\"Ticker\"] == tkr].sort_values(\"Date\")\n",
    "    if sub.empty:\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(sub[\"Date\"], sub[\"TrueTarget\"], lw=2.0, label=\"True Target\")\n",
    "    ax.plot(sub[\"Date\"], sub[\"PredTarget\"], lw=2.0, label=\"Predicted Target\")\n",
    "\n",
    "    ax.set_title(f\"{tkr}: Actual and Predicted Target, Per-Ticker Linear Regression\",\n",
    "                 fontsize=20, weight=\"bold\")\n",
    "    ax.set_ylabel(\"Target\", fontsize=25)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.tick_params(axis=\"x\", colors=\"black\", labelsize=15)\n",
    "    ax.tick_params(axis=\"y\", colors=\"black\", labelsize=18)\n",
    "    ax.legend(loc=\"upper right\", frameon=True, fontsize=20)\n",
    "    ax.set_facecolor(\"white\")\n",
    "    ax.grid(True, which=\"major\", linestyle=\"-\", alpha=0.15)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b73c679",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_sizes, dropout=0.0, batchnorm=False, spectral_norm=False):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d = in_dim\n",
    "        for h in hidden_sizes:\n",
    "            linear = nn.Linear(d, h)\n",
    "            if spectral_norm:\n",
    "                linear = nn.utils.spectral_norm(linear)\n",
    "            block = [linear]\n",
    "            if batchnorm:\n",
    "                block.append(nn.BatchNorm1d(h))\n",
    "            block += [nn.ReLU()]\n",
    "            if dropout and dropout > 0:\n",
    "                block.append(nn.Dropout(p=dropout))\n",
    "            layers += block\n",
    "            d = h\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d, 1),\n",
    "            nn.Softplus(beta=1.0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x))\n",
    "\n",
    "\n",
    "#Elastic penalties (where we exclude biases and normalisation parameters)\n",
    "def elastic_penalty(model, l1=0.0, l2=0.0):\n",
    "    if l1 == 0.0 and l2 == 0.0:\n",
    "        return torch.zeros((), device=next(model.parameters()).device)\n",
    "    l1_term = torch.zeros((), device=next(model.parameters()).device)\n",
    "    l2_term = torch.zeros((), device=next(model.parameters()).device)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            W = m.weight\n",
    "            if l1 > 0:\n",
    "                l1_term = l1_term + W.abs().sum()\n",
    "            if l2 > 0:\n",
    "                l2_term = l2_term + (W * W).sum()\n",
    "    return l1 * l1_term + l2 * l2_term\n",
    "\n",
    "@torch.no_grad()\n",
    "def dmse_score(model, X, y, over_penalty=9):\n",
    "    model.eval()\n",
    "    xb = torch.as_tensor(X, dtype=torch.float32, device=device)\n",
    "    yb = torch.as_tensor(y, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "    preds = model(xb)\n",
    "    diff2 = (yb - preds) ** 2\n",
    "    loss = torch.where(preds <= yb, diff2, diff2 * over_penalty).mean()\n",
    "    return float(loss.item())\n",
    "\n",
    "\n",
    "#This is a helper function to perform the validation split\n",
    "\n",
    "def chronological_split(X, y, ticker_ids, val_frac=0.2):\n",
    "    \"\"\"\n",
    "    For each ticker (where dates are sorted chronologically),\n",
    "    take the last val_frac of *each* ticker as validation.\n",
    "    \"\"\"\n",
    "    ticker_ids = np.asarray(ticker_ids)\n",
    "    idx_tr, idx_va = [], []\n",
    "    for t in np.unique(ticker_ids):\n",
    "        idx = np.flatnonzero(ticker_ids == t)  #contiguous block\n",
    "        k = int(round(len(idx) * (1.0 - val_frac)))\n",
    "        idx_tr.append(idx[:k])\n",
    "        idx_va.append(idx[k:])\n",
    "    idx_tr = np.concatenate(idx_tr); idx_va = np.concatenate(idx_va)\n",
    "    return (X[idx_tr], y[idx_tr]), (X[idx_va], y[idx_va])\n",
    "\n",
    "def make_adamw(model, lr, weight_decay):\n",
    "    decay, no_decay = [], []\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            decay.append(module.weight)     #decay on weights\n",
    "            if module.bias is not None:\n",
    "                no_decay.append(module.bias)\n",
    "        elif isinstance(module, (nn.BatchNorm1d, nn.LayerNorm)):\n",
    "            no_decay += list(module.parameters())\n",
    "    names = {id(p): n for n, p in model.named_parameters()}\n",
    "    for p in model.parameters():\n",
    "        if id(p) not in map(id, decay + no_decay):\n",
    "            no_decay.append(p)\n",
    "    return torch.optim.AdamW(\n",
    "        [{\"params\": decay,    \"weight_decay\": weight_decay, \"lr\": lr},\n",
    "         {\"params\": no_decay, \"weight_decay\": 0.0,          \"lr\": lr}]\n",
    "    )\n",
    "\n",
    "#Training\n",
    "def train_one_model(\n",
    "    X_train_np, y_train_np, ticker_ids_train=None,\n",
    "    hidden_sizes=(512, 256, 128, 64),\n",
    "    dropout=0.0,\n",
    "    batchnorm=False,\n",
    "    spectral_norm=False,\n",
    "    input_noise_std=0.0,\n",
    "    #optimisation\n",
    "    epochs=80,\n",
    "    batch_size=1024,\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.0,   \n",
    "    l1=0.0,        \n",
    "    l2=0.0,           \n",
    "    patience=15,\n",
    "    over_penalty=9,\n",
    "    seed=0,\n",
    "    val_frac=0.2):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    n_features = X_train_np.shape[1]\n",
    "\n",
    "    (Xtr_np, ytr_np), (Xva_np, yva_np) = chronological_split(\n",
    "        X_train_np, y_train_np, ticker_ids_train, val_frac=val_frac)\n",
    "\n",
    "    use_val = (Xva_np.size > 0) \n",
    "\n",
    "    Xtr = torch.as_tensor(Xtr_np, dtype=torch.float32, device=device)\n",
    "    ytr = torch.as_tensor(ytr_np, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "    if use_val:\n",
    "        Xva = torch.as_tensor(Xva_np, dtype=torch.float32, device=device)\n",
    "        yva = torch.as_tensor(yva_np, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "\n",
    "    #no shuffling, and don't drop the last partial batch\n",
    "    train_dl = DataLoader(TensorDataset(Xtr, ytr), batch_size=batch_size,\n",
    "                          shuffle=False, drop_last=False) \n",
    "\n",
    "    model = MLP(n_features, hidden_sizes, dropout, batchnorm, spectral_norm).to(device)\n",
    "    criterion = DMSE(over_penalty=over_penalty)\n",
    "    optim = make_adamw(model, lr=lr, weight_decay=weight_decay)\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optim, mode=\"min\", factor=0.5,\n",
    "        patience=max(3, patience // 3), verbose=False\n",
    "    )\n",
    "\n",
    "    best_val, best_state = float(\"inf\"), None\n",
    "    history = {\"epoch\": [], \"train_dmse\": [], \"val_dmse\": [], \"lr\": []}\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            xb_aug = xb + torch.randn_like(xb) * input_noise_std if input_noise_std > 0 else xb\n",
    "            preds = model(xb_aug)\n",
    "            loss = criterion(preds, yb)\n",
    "            if l1 > 0 or l2 > 0:\n",
    "                loss = loss + elastic_penalty(model, l1=l1, l2=l2)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            train_dmse = dmse_score(model, Xtr_np, ytr_np, over_penalty=over_penalty)\n",
    "            if use_val:\n",
    "                val_dmse = dmse_score(model, Xva_np, yva_np, over_penalty=over_penalty)\n",
    "                metric = val_dmse\n",
    "            else:\n",
    "                val_dmse = float(\"nan\")\n",
    "                metric = train_dmse  \n",
    "\n",
    "            history[\"epoch\"].append(epoch)\n",
    "            history[\"train_dmse\"].append(train_dmse)\n",
    "            history[\"val_dmse\"].append(val_dmse)\n",
    "            history[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "\n",
    "            sched.step(metric)\n",
    "\n",
    "            if use_val:\n",
    "                if val_dmse + 1e-12 < best_val:\n",
    "                    best_val = val_dmse\n",
    "                    best_state = deepcopy(model.state_dict())\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    break\n",
    "\n",
    "    if use_val and best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    if use_val:\n",
    "        with torch.no_grad():\n",
    "            yva_hat = model(torch.as_tensor(Xva_np, dtype=torch.float32, device=device)).cpu().numpy().ravel()\n",
    "        yva_mean = yva_np.mean() if yva_np.size else 0.0\n",
    "        mean_ratio = float(yva_hat.mean() / (yva_mean + 1e-8))\n",
    "        pred_std   = float(yva_hat.std())\n",
    "        score_out  = best_val\n",
    "    else:\n",
    "        mean_ratio = float(\"nan\")\n",
    "        pred_std   = float(\"nan\")\n",
    "        score_out  = train_dmse \n",
    "\n",
    "    return model, history, score_out, {\"mean_ratio\": mean_ratio, \"pred_std\": pred_std}\n",
    "\n",
    "\n",
    "#This runs the experiments\n",
    "def run_all(X_train_stand_np, y_train_np, X_test_stand_np, y_test_np, over_penalty=9, ticker_ids_train=None):\n",
    "    configs = [\n",
    "        (\"SpectralNorm+Dropout\", dict(dropout=0.20, batchnorm=False, spectral_norm=True,\n",
    "                                      input_noise_std=0.0, lr=1e-3, weight_decay=5e-4,\n",
    "                                      l1=0.0, l2=0.0, hidden_sizes=(512,256,128,64), patience=15, epochs=80)),\n",
    "        (\"ElasticNet+Dropout\",   dict(dropout=0.30, batchnorm=False, spectral_norm=False,\n",
    "                                      input_noise_std=0.0, lr=1e-3, weight_decay=0.0,\n",
    "                                      l1=1e-6, l2=1e-5, hidden_sizes=(512,256,128,64), patience=15, epochs=80)),\n",
    "        (\"L2+Dropout+BatchNorm\", dict(dropout=0.25, batchnorm=True,  spectral_norm=False,\n",
    "                                      input_noise_std=0.0, lr=1e-3, weight_decay=1e-3,\n",
    "                                      l1=0.0, l2=0.0, hidden_sizes=(512,256,128,64), patience=15, epochs=80)),\n",
    "        (\"InputNoise+L2\",        dict(dropout=0.00, batchnorm=False, spectral_norm=False,\n",
    "                                      input_noise_std=0.05, lr=1e-3, weight_decay=1e-3,\n",
    "                                      l1=0.0, l2=0.0, hidden_sizes=(512,256,128,64), patience=15, epochs=80)),\n",
    "    ]\n",
    "\n",
    "    models = {}\n",
    "    rows = []\n",
    "    for name, kwargs in configs:\n",
    "        model, history, val_dmse, diags = train_one_model(X_train_stand_np, y_train_np, \n",
    "                                          ticker_ids_train=ticker_ids_train, over_penalty=over_penalty, **kwargs)\n",
    "        models[name] = {\"model\": model, \"history\": history, \"val_dmse\": val_dmse, **diags}\n",
    "        rows.append((name, val_dmse, abs(diags[\"mean_ratio\"] - 1.0), -diags[\"pred_std\"]))\n",
    "\n",
    "\n",
    "    #Rank by validation DMSE, then prefer mean close to true mean, then larger prediction spread\n",
    "    rows.sort(key=lambda t: (t[1], t[2], t[3]))\n",
    "    best_name, best_val = rows[0][0], rows[0][1]\n",
    "\n",
    "    #Retrain on the full 7-year window with the chosen hyperparameters\n",
    "    best_cfg = {name: cfg for (name, cfg) in configs}[best_name]\n",
    "    best_model, _, _train_dmse_full, _ = train_one_model(\n",
    "    X_train_stand_np, y_train_np,\n",
    "    ticker_ids_train=ticker_ids_train,\n",
    "    over_penalty=over_penalty,\n",
    "    val_frac=0.0,                #no hold-out\n",
    "    **best_cfg\n",
    ")\n",
    "\n",
    "    test_dmse = dmse_score(best_model, X_test_stand_np, y_test_np, over_penalty=over_penalty)\n",
    "\n",
    "    df_results = pd.DataFrame({\n",
    "    \"config\":   [n for (n, _, _, _) in rows],\n",
    "    \"val_DMSE\": [models[n][\"val_dmse\"] for (n, _, _, _) in rows],\n",
    "    \"mean_ratio\": [models[n][\"mean_ratio\"] for (n, _, _, _) in rows],\n",
    "    \"pred_std\":   [models[n][\"pred_std\"]   for (n, _, _, _) in rows],\n",
    "    }).sort_values(\"val_DMSE\").reset_index(drop=True)\n",
    "\n",
    "    df_results[\"test_DMSE_best\"] = None\n",
    "    df_results.loc[df_results[\"config\"] == best_name, \"test_DMSE_best\"] = test_dmse\n",
    "\n",
    "    print(f\"Selected best config: {best_name}  |  val DMSE = {best_val:.5f}  |  test DMSE = {test_dmse:.5f}\")\n",
    "    return best_model, best_name, df_results, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ba0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_ids_train = df_train[\"Ticker\"].to_numpy()\n",
    "\n",
    "best_model, best_name, df_results, models = run_all(\n",
    "    X_train_stand_np, y_train_np, X_test_stand_np, y_test_np,\n",
    "    over_penalty=9,\n",
    "    ticker_ids_train=ticker_ids_train)\n",
    "\n",
    "best_model.eval() \n",
    "with torch.no_grad():\n",
    "    xb = torch.as_tensor(X_test_stand_np, dtype=torch.float32, device=device)\n",
    "    preds = best_model(xb).cpu().numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea7273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per-ticker plots using predictions from the global feedforward neural network\n",
    "\n",
    "plot_df_global = df_test.loc[:, [\"Ticker\", \"Date\"]].copy()\n",
    "plot_df_global[\"TrueTarget\"] = y_test_np.ravel()\n",
    "plot_df_global[\"PredTarget_GlobalMLP\"] = np.asarray(preds).ravel()\n",
    "\n",
    "for tkr in tickers[10:20]:\n",
    "    sub = plot_df_global.loc[plot_df_global[\"Ticker\"] == tkr].sort_values(\"Date\")\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(sub[\"Date\"], sub[\"TrueTarget\"], lw=2.0, label=\"True Target\")\n",
    "    ax.plot(sub[\"Date\"], sub[\"PredTarget_GlobalMLP\"], lw=2.0, label=\"Predicted Target\")\n",
    "\n",
    "    ax.set_title(f\"{tkr}: Actual and Predicted Target, Feedforward Neural Network Trained on all Stocks\", \n",
    "                 fontsize=20, weight=\"bold\")\n",
    "    ax.set_ylabel(\"Target\", fontsize=25)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.tick_params(axis=\"x\", colors=\"black\", labelsize=15)\n",
    "    ax.tick_params(axis=\"y\", colors=\"black\", labelsize=18)\n",
    "    ax.legend(loc=\"upper right\", frameon=True, fontsize=20)\n",
    "    ax.set_facecolor(\"white\")\n",
    "    ax.grid(True, which=\"major\", linestyle=\"-\", alpha=0.15)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_ticker_r = (\n",
    "    plot_df_global.dropna(subset=[\"TrueTarget\",\"PredTarget_GlobalMLP\"])\n",
    "    .groupby(\"Ticker\")\n",
    "    .apply(lambda d: d[\"TrueTarget\"].corr(d[\"PredTarget_GlobalMLP\"]))\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "print(per_ticker_r.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23120a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_ticker_models = {}\n",
    "N_test = len(df_test)\n",
    "y_pred_single_mlp_all = np.full(N_test, np.nan, dtype=np.float32)\n",
    "\n",
    "for tkr in tickers[10:20]:\n",
    "    #indices for this ticker in train/test (aligned to X_*_stand_np / y_*_np)\n",
    "    idx_tr = np.flatnonzero(df_train[\"Ticker\"].values == tkr)\n",
    "    idx_te = np.flatnonzero(df_test[\"Ticker\"].values  == tkr)\n",
    "    if len(idx_tr) == 0 or len(idx_te) == 0:\n",
    "        continue\n",
    "\n",
    "    #slice arrays\n",
    "    X_tr = X_train_stand_np[idx_tr, :]\n",
    "    y_tr = y_train_np[idx_tr]\n",
    "    X_te = X_test_stand_np[idx_te, :]\n",
    "    y_te = y_test_np[idx_te]\n",
    "\n",
    "    #ticker ids for chronological_split inside train_one_model/run_all\n",
    "    ticker_ids_train_tkr = df_train[\"Ticker\"].values[idx_tr]  #array of the same ticker\n",
    "    \n",
    "    best_model_tkr, best_name_tkr, df_results_tkr, models_tkr = run_all(\n",
    "        X_tr, y_tr, X_te, y_te,\n",
    "        over_penalty=9,\n",
    "        ticker_ids_train=ticker_ids_train_tkr\n",
    "    )\n",
    "\n",
    "    per_ticker_models[tkr] = {\n",
    "        \"model\": best_model_tkr,\n",
    "        \"best_name\": best_name_tkr,\n",
    "        \"df_results\": df_results_tkr,\n",
    "        \"models\": models_tkr\n",
    "    }\n",
    "    \n",
    "    best_model_tkr.eval()\n",
    "    with torch.no_grad():\n",
    "        xb_te = torch.as_tensor(X_te, dtype=torch.float32, device=device)\n",
    "        preds_te = best_model_tkr(xb_te).cpu().numpy().ravel().astype(np.float32)\n",
    "\n",
    "    #place back in global test order\n",
    "    y_pred_single_mlp_all[idx_te] = preds_te\n",
    "\n",
    "\n",
    "mask = ~np.isnan(y_pred_single_mlp_all)\n",
    "y_true_all = y_test_np[mask].reshape(-1, 1)\n",
    "y_pred_all = y_pred_single_mlp_all[mask].reshape(-1, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    dmse_aggregate = DMSE(over_penalty=9)(\n",
    "        torch.as_tensor(y_pred_all, dtype=torch.float32, device=device),\n",
    "        torch.as_tensor(y_true_all, dtype=torch.float32, device=device)\n",
    "    ).item()\n",
    "print(f\"(Per-ticker MLP) Aggregate DMSE over the 10 tickers' test samples: {dmse_aggregate:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8409d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df_single = df_test.loc[:, [\"Ticker\", \"Date\"]].copy()\n",
    "plot_df_single[\"TrueTarget\"] = y_test_np.ravel()\n",
    "plot_df_single[\"PredTarget_PerTickerMLP\"] = y_pred_single_mlp_all\n",
    "\n",
    "for tkr in tickers[10:20]:\n",
    "    sub = plot_df_single.loc[plot_df_single[\"Ticker\"] == tkr].sort_values(\"Date\")\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(sub[\"Date\"], sub[\"TrueTarget\"], lw=2.0, label=\"True Target\")\n",
    "    ax.plot(sub[\"Date\"], sub[\"PredTarget_PerTickerMLP\"], lw=2.0, label=\"Predicted Target\")\n",
    "\n",
    "    ax.set_title(f\"{tkr}: Actual and Predicted Target, Per-Ticker Feedforward Neural Network\", \n",
    "                 fontsize=20, weight=\"bold\")\n",
    "    ax.set_ylabel(\"Target\", fontsize=25)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.tick_params(axis=\"x\", colors=\"black\", labelsize=15)\n",
    "    ax.tick_params(axis=\"y\", colors=\"black\", labelsize=18)\n",
    "    ax.legend(loc=\"upper right\", frameon=True, fontsize=20)\n",
    "    ax.set_facecolor(\"white\")\n",
    "    ax.grid(True, which=\"major\", linestyle=\"-\", alpha=0.15)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34771d",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94f42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the random forest model, we use non-standardised data\n",
    "X_train = df_train[predictors]\n",
    "X_test = df_test[predictors]\n",
    "\n",
    "#We fit a full random forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=1, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#This computes feature importance based on the reduction in impurity attributable to each variable\n",
    "#This is Table 4\n",
    "importance_df_rf = pd.DataFrame({\"Predictor\": predictors, \"Importance\": rf.feature_importances_})\n",
    "importance_df_rf.sort_values(by=\"Importance\", ascending=False, inplace=True)\n",
    "importance_df_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32cbded",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_predictors_rf = list(importance_df_rf[\"Predictor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = list(importance_df_rf[\"Importance\"])\n",
    "sum(imp[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1c7ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.to_numpy(dtype=np.float32, copy=False)\n",
    "X_test_np  = X_test.to_numpy(dtype=np.float32,  copy=False)\n",
    "y_train_np = y_train.to_numpy(dtype=np.float32, copy=False)\n",
    "y_test_np  = y_test.to_numpy(dtype=np.float32,  copy=False)\n",
    "\n",
    "col_to_idx = {c: i for i, c in enumerate(predictors)}\n",
    "\n",
    "criterion = DMSE() \n",
    "y_test_tensor = torch.as_tensor(y_test_np.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "\n",
    "performance_rf = {}\n",
    "\n",
    "for k in range(1, len(sorted_predictors_rf) + 1):\n",
    "    selected_variables = sorted_predictors_rf[:k]\n",
    "    idx = [col_to_idx[c] for c in selected_variables]\n",
    "\n",
    "    X_rf_train_subset = X_train_np[:, idx]\n",
    "    X_rf_test_subset  = X_test_np[:, idx]\n",
    "\n",
    "    temporary_model = RandomForestRegressor(n_estimators=100, random_state=1, n_jobs=-1)\n",
    "    temporary_model.fit(X_rf_train_subset, y_train_np)\n",
    "\n",
    "    temporary_y_pred = temporary_model.predict(X_rf_test_subset)\n",
    "\n",
    "    y_pred_tensor = torch.as_tensor(temporary_y_pred.reshape(-1, 1), dtype=torch.float32)\n",
    "    performance_rf[k] = criterion(y_pred_tensor, y_test_tensor).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e0fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates Figure 13\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5)) \n",
    "fig.patch.set_facecolor(\"white\") \n",
    "\n",
    "plt.plot(list(performance_rf.keys()), list(performance_rf.values()), color=\"magenta\")       \n",
    "plt.xlabel(\"Number of Predictors\", fontsize=20)\n",
    "plt.ylabel(\"DMSE\", fontsize=20)\n",
    "plt.title(f\"Out-of-Sample DMSE of Random Forest as a Function of Number of Predictors\", \n",
    "          fontsize=13, weight=\"bold\")                         \n",
    "plt.grid(color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
    "ax = plt.gca() \n",
    "ax.set_facecolor(\"white\") \n",
    "ax.tick_params(axis=\"x\", colors=\"black\", labelsize=13) \n",
    "ax.tick_params(axis=\"y\", colors=\"black\", labelsize=16)  \n",
    "ax.set_xticks(range(1, len(sorted_predictors_rf)+1))\n",
    "for spine in ax.spines.values(): \n",
    "    spine.set_edgecolor(\"black\") \n",
    "    spine.set_linewidth(1)\n",
    "\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29812606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We start by training, on the first 7 years, a global random forest which includes all stocks (and predictors).\n",
    "#We then test it on the following 2 years\n",
    "\n",
    "rf_global_model = RandomForestRegressor(\n",
    "    n_estimators=100, random_state=1, n_jobs=-1 \n",
    ")\n",
    "rf_global_model.fit(X_train_np, y_train_np)\n",
    "\n",
    "#Predict on the test set\n",
    "rf_global_pred = rf_global_model.predict(X_test_np).astype(np.float32)\n",
    "\n",
    "#DMSE on the test set\n",
    "y_pred_tensor = torch.as_tensor(rf_global_pred.reshape(-1, 1), dtype=torch.float32)\n",
    "rf_global_dmse = criterion(y_pred_tensor, y_test_tensor).item()\n",
    "print(f\"Global RF (all predictors, all stocks) — Test DMSE: {rf_global_dmse:.5f}\")\n",
    "\n",
    "#Per-ticker time-series plots using the global RF predictions\n",
    "plot_df_rf_global = df_test.loc[:, [\"Ticker\", \"Date\"]].copy()\n",
    "plot_df_rf_global[\"TrueTarget\"] = y_test_np.ravel()\n",
    "plot_df_rf_global[\"PredTarget_RF_Global\"] = rf_global_pred.ravel()\n",
    "\n",
    "for tkr in tickers[20:]:\n",
    "    sub = plot_df_rf_global.loc[plot_df_rf_global[\"Ticker\"] == tkr].sort_values(\"Date\")\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(sub[\"Date\"], sub[\"TrueTarget\"], lw=2.0, label=\"True Target\")\n",
    "    ax.plot(sub[\"Date\"], sub[\"PredTarget_RF_Global\"], lw=2.0, label=\"Predicted Target\")\n",
    "    \n",
    "    ax.set_title(f\"{tkr}: Actual and Predicted Target, Random Forest Trained on all Stocks\", \n",
    "                 fontsize=20, weight=\"bold\")\n",
    "    ax.set_ylabel(\"Target\", fontsize=25)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.tick_params(axis=\"x\", colors=\"black\", labelsize=15)\n",
    "    ax.tick_params(axis=\"y\", colors=\"black\", labelsize=18)\n",
    "    ax.legend(loc=\"upper right\", frameon=True, fontsize=20)\n",
    "    ax.set_facecolor(\"white\")\n",
    "    ax.grid(True, which=\"major\", linestyle=\"-\", alpha=0.15)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b4998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now train a different random forest model for each stock, performing the same tests as before\n",
    "\n",
    "rf_models_by_ticker = {}\n",
    "\n",
    "N_test = len(df_test)\n",
    "rf_per_ticker_pred_all = np.full(N_test, np.nan, dtype=np.float32)\n",
    "\n",
    "for tkr in tickers[20:]:\n",
    "    idx_tr = np.flatnonzero(df_train[\"Ticker\"].values == tkr)\n",
    "    idx_te = np.flatnonzero(df_test[\"Ticker\"].values  == tkr)\n",
    "    if len(idx_tr) == 0 or len(idx_te) == 0:\n",
    "        continue\n",
    "\n",
    "    X_tr_tkr = X_train_np[idx_tr, :]\n",
    "    y_tr_tkr = y_train_np[idx_tr]\n",
    "    X_te_tkr = X_test_np[idx_te, :]\n",
    "\n",
    "    #Train a fresh RF for this ticker\n",
    "    rf_tkr = RandomForestRegressor(\n",
    "        n_estimators=100, random_state=1, n_jobs=-1\n",
    "    )\n",
    "    rf_tkr.fit(X_tr_tkr, y_tr_tkr)\n",
    "    rf_models_by_ticker[tkr] = rf_tkr\n",
    "\n",
    "    #Predict on this ticker's test rows\n",
    "    yhat_tkr = rf_tkr.predict(X_te_tkr).astype(np.float32)\n",
    "\n",
    "    #Place predictions back into the global array using the test indices\n",
    "    rf_per_ticker_pred_all[idx_te] = yhat_tkr\n",
    "\n",
    "#Overall DMSE computed only on the last 10 tickers\n",
    "mask_last10 = np.array(pd.Index(df_test[\"Ticker\"]).isin(tickers[20:])).astype(bool)\n",
    "\n",
    "y_pred_subset = rf_per_ticker_pred_all[mask_last10]\n",
    "y_true_subset = y_test_np[mask_last10]\n",
    "\n",
    "y_pred_tensor_pt = torch.as_tensor(y_pred_subset.reshape(-1, 1), dtype=torch.float32)\n",
    "y_true_tensor_pt = torch.as_tensor(y_true_subset.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "rf_per_ticker_dmse = criterion(y_pred_tensor_pt, y_true_tensor_pt).item()\n",
    "print(f\"Per-ticker RFs (last 10 tickers) — Test DMSE: {rf_per_ticker_dmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdca7b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now produce the same plots as before, using the per-ticker random forests\n",
    "\n",
    "plot_df_rf_single = df_test.loc[:, [\"Ticker\", \"Date\"]].copy()\n",
    "plot_df_rf_single[\"TrueTarget\"] = y_test_np.ravel()\n",
    "plot_df_rf_single[\"PredTarget_RF_PerTicker\"] = rf_per_ticker_pred_all.ravel()\n",
    "\n",
    "for tkr in tickers[20:]:\n",
    "    sub = plot_df_rf_single.loc[plot_df_rf_single[\"Ticker\"] == tkr].sort_values(\"Date\")\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(sub[\"Date\"], sub[\"TrueTarget\"], lw=2.0, label=\"True Target\")\n",
    "    ax.plot(sub[\"Date\"], sub[\"PredTarget_RF_PerTicker\"], lw=2.0, label=\"Predicted Target\")\n",
    "    \n",
    "    ax.set_title(f\"{tkr}: Actual and Predicted Target, Per-Ticker Random Forest\", \n",
    "                 fontsize=20, weight=\"bold\")\n",
    "    ax.set_ylabel(\"Target\", fontsize=25)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.tick_params(axis=\"x\", colors=\"black\", labelsize=15)\n",
    "    ax.tick_params(axis=\"y\", colors=\"black\", labelsize=18)\n",
    "    ax.legend(loc=\"upper right\", frameon=True, fontsize=20)\n",
    "    ax.set_facecolor(\"white\")\n",
    "    ax.grid(True, which=\"major\", linestyle=\"-\", alpha=0.15)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b5118",
   "metadata": {},
   "source": [
    "### Autoregressive Moving-Average Integrated Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6192c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA/ARIMAX via H reduced-frequency subseries\n",
    "\n",
    "H = 10  #horizon\n",
    "\n",
    "#Some DMSE helper functions\n",
    "def dmse_np(y_true, y_pred, alpha=9.0):\n",
    "    y_true = np.asarray(y_true, dtype=float).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=float).ravel()\n",
    "    diff2 = (y_true - y_pred) ** 2\n",
    "    w = (y_pred > y_true).astype(float) * (alpha - 1.0) + 1.0\n",
    "    return float(np.mean(diff2 * w))\n",
    "\n",
    "def mse_np(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=float).ravel()\n",
    "    return float(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "#Some fitting helper functions\n",
    "def _fit_sarimax(endog, exog, order, *, trend=None):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        trend_use = \"c\" if order[1] == 0 else \"n\"\n",
    "        mod = SARIMAX(\n",
    "            endog=endog, exog=exog, order=order,\n",
    "            seasonal_order=(0,0,0,0), trend=trend_use,\n",
    "            enforce_stationarity=False, enforce_invertibility=False,\n",
    "            measurement_error=False, mle_regression=True,\n",
    "            missing=\"drop\"              \n",
    "        )\n",
    "        res = mod.fit(disp=False, maxiter=50, method=\"lbfgs\")  \n",
    "    return res\n",
    "\n",
    "def _bic_of_order_dmse(y_tr, x_tr, order, alpha):\n",
    "    \"\"\"\n",
    "    DMSE-based BIC for a candidate ARIMA/ARIMAX order on the reduced-frequency subseries.\n",
    "    We fit (Gaussian) SARIMAX to get 1-step-ahead in-sample predictions, compute residuals,\n",
    "    evaluate the DMSE log-likelihood, and then form BIC = -2*ll + k*log(n),\n",
    "    with k counting ALL estimated params (AR, MA, intercept, exog, variance).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res = _fit_sarimax(y_tr, x_tr, order)  \n",
    "        yhat = np.asarray(res.fittedvalues, dtype=float)  \n",
    "        resid = np.asarray(y_tr, dtype=float) - yhat\n",
    "        ll = dmse_loglik(resid, alpha=alpha)  \n",
    "        k = int(res.params.size)               #includes intercept, AR, MA, exog, sigma2\n",
    "        n = resid.size\n",
    "        bic = -2.0 * ll + k * np.log(n)\n",
    "        return float(bic) if np.isfinite(bic) else np.inf\n",
    "    except Exception:\n",
    "        return np.inf\n",
    "\n",
    "def _initial_order_bic(y_tr, x_tr=None, alpha=9.0):\n",
    "    grid = [(p, d, q) for p in range(4) for d in (0, 1) for q in range(4)]\n",
    "    best, best_bic = None, np.inf\n",
    "    for od in grid:\n",
    "        bic = _bic_of_order_dmse(y_tr, x_tr, od, alpha)\n",
    "        if bic < best_bic:\n",
    "            best_bic, best = bic, od\n",
    "    return best if best is not None else (0, 0, 0)\n",
    "\n",
    "def _local_grid_around(p0, d0, q0):\n",
    "    P = [x for x in {p0-1, p0, p0+1} if 0 <= x <= 2]   #cap at 2\n",
    "    Q = [x for x in {q0-1, q0, q0+1} if 0 <= x <= 2]\n",
    "    D = [0, 1]\n",
    "    return [(p, d, q) for p in P for d in D for q in Q]\n",
    "\n",
    "def _fit_and_forecast(y_tr_sub, x_tr_sub, y_va, x_va, order, use_exog):\n",
    "    \"\"\"\n",
    "    Fit on y_tr_sub (optionally with x_tr_sub) and one-step-ahead forecast\n",
    "    the validation horizon len(y_va) on the reduced-frequency index.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res = _fit_sarimax(y_tr_sub, (x_tr_sub if use_exog else None), order)\n",
    "        res2 = res.apply(endog=y_va, exog=(x_va if use_exog else None), refit=False)\n",
    "        yhat = np.asarray(res2.fittedvalues, dtype=float)[-len(y_va):]  #1-step ahead for appended\n",
    "        if yhat.shape[0] != len(y_va):\n",
    "            return None, None\n",
    "        return yhat, res\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "#Core routine for one ticker\n",
    "def _predict_one_ticker(df_tr_tkr, df_te_tkr, predictors, H=10, alpha=9.0, val_frac=0.10):\n",
    "    \"\"\"\n",
    "    -Initial order guess on each remainder-class subseries via BIC.\n",
    "    -Local grid around (p0,d0,q0).\n",
    "    -Internal DMSE validation (last 10% of the training subseries). For each candidate order, fit both \n",
    "    ARIMA (no exog) and ARIMAX (with exog). Keep the (order, model_type) with lowest DMSE; fall back to \n",
    "    (0,0,0) if needed.\n",
    "    -Refit on entire training subseries with the selected choice and forecast the test subseries. Recombine \n",
    "    H subseries back to raw test dates.\n",
    "    \"\"\"\n",
    "    if df_te_tkr.empty:\n",
    "        return np.array([], dtype=float), pd.Series([], dtype=\"datetime64[ns]\")\n",
    "\n",
    "    #Build a raw-time index t = 0..(n_all-1) across train+test (sorted by Date).\n",
    "    df_all = pd.concat([df_tr_tkr, df_te_tkr], axis=0, ignore_index=True).sort_values(\"Date\")\n",
    "    df_all = df_all.reset_index(drop=True)\n",
    "    df_all[\"t_raw\"] = np.arange(len(df_all), dtype=int)\n",
    "    #Map raw index back to each split\n",
    "    df_tr = df_all.iloc[:len(df_tr_tkr)]\n",
    "    df_te = df_all.iloc[len(df_tr_tkr):]\n",
    "\n",
    "    #Pre-extract arrays\n",
    "    y_tr_all = df_tr[\"Target\"].to_numpy(dtype=float)\n",
    "    y_te_all = df_te[\"Target\"].to_numpy(dtype=float)\n",
    "    X_tr_all = df_tr[predictors].to_numpy(dtype=float) if predictors else None\n",
    "    X_te_all = df_te[predictors].to_numpy(dtype=float) if predictors else None\n",
    "    t_tr = df_tr[\"t_raw\"].to_numpy()\n",
    "    t_te = df_te[\"t_raw\"].to_numpy()\n",
    "\n",
    "    #Container for test predictions (aligned to df_te order)\n",
    "    yhat_te = np.full(shape=(len(df_te),), fill_value=np.nan, dtype=float)\n",
    "    bic_vals = []\n",
    "    val_mse_vals = []\n",
    "\n",
    "    #Work remainder class by remainder class\n",
    "    for r in range(H):\n",
    "        #Indices in reduced-frequency subseries (training)\n",
    "        idx_tr_r = np.flatnonzero(t_tr % H == r)\n",
    "        if idx_tr_r.size == 0:\n",
    "            continue\n",
    "        y_tr_r = y_tr_all[idx_tr_r]\n",
    "        X_tr_r = None if X_tr_all is None else X_tr_all[idx_tr_r, :]\n",
    "\n",
    "        #Initial order via BIC on the whole training subseries y^(r)\n",
    "        p0, d0, q0 = _initial_order_bic(y_tr_r, None, alpha=alpha)\n",
    "\n",
    "        #Local grid\n",
    "        candidates = _local_grid_around(p0, d0, q0)\n",
    "\n",
    "        #Validation split on the subseries (last 10% as validation)\n",
    "        n = len(y_tr_r)\n",
    "        n_va = max(1, int(np.floor(val_frac * n)))\n",
    "        n_tr_sub = max(1, n - n_va)\n",
    "        y_tr_sub, y_va = y_tr_r[:n_tr_sub], y_tr_r[n_tr_sub:]\n",
    "        X_tr_sub = None if X_tr_r is None else X_tr_r[:n_tr_sub, :]\n",
    "        X_va = None if X_tr_r is None else X_tr_r[n_tr_sub:, :]\n",
    "\n",
    "        #Internal DMSE validation: try ARIMA and ARIMAX for each order\n",
    "        best_tuple = None  #(dmse, order, use_exog, fitted_result_for_refit_is_irrelevant)\n",
    "        for od in candidates:\n",
    "            #ARIMA (no exog)\n",
    "            yhat_a, _ = _fit_and_forecast(y_tr_sub, None, y_va, None, od, use_exog=False)\n",
    "            if yhat_a is not None:\n",
    "                dmse_a = mse_np(y_va, yhat_a)  #validation uses MSE\n",
    "                if (best_tuple is None) or (dmse_a < best_tuple[0] - 1e-10) or \\\n",
    "                   (abs(dmse_a - best_tuple[0]) <= 1e-10 and best_tuple[2] is True):\n",
    "                    best_tuple = (dmse_a, od, False)\n",
    "            #ARIMAX (with contemporaneous X)\n",
    "            if X_tr_sub is not None:\n",
    "                yhat_x, _ = _fit_and_forecast(y_tr_sub, X_tr_sub, y_va, X_va, od, use_exog=True)\n",
    "                if yhat_x is not None:\n",
    "                    dmse_x = mse_np(y_va, yhat_x)  #validation uses MSE\n",
    "                    if (best_tuple is None) or (dmse_x < best_tuple[0] - 1e-10) or \\\n",
    "                       (abs(dmse_x - best_tuple[0]) <= 1e-10 and best_tuple[2] is False):\n",
    "                        best_tuple = (dmse_x, od, True)  #prefer exog on ties  \n",
    "\n",
    "        #Fallback if nothing converged: try (0,0,0) without exog; if still failing, use mean\n",
    "        if best_tuple is None:\n",
    "            od = (0, 0, 0)\n",
    "            yhat_try, _ = _fit_and_forecast(y_tr_sub, None, y_va, None, od, use_exog=False)\n",
    "            if yhat_try is None:\n",
    "                best_tuple = (mse_np(y_va, np.full_like(y_va, y_tr_sub.mean())), od, False)\n",
    "            else:\n",
    "                best_tuple = (mse_np(y_va, yhat_try), od, False)\n",
    "\n",
    "        val_mse_vals.append(best_tuple[0])   #store chosen validation error (MSE)\n",
    "        _, best_order, best_use_exog = best_tuple\n",
    "\n",
    "        #Final refit on full training subseries and forecast this remainder's test subseries\n",
    "        idx_te_r = np.flatnonzero(t_te % H == r)\n",
    "        if idx_te_r.size == 0:\n",
    "            continue\n",
    "\n",
    "        y_te_r = y_te_all[idx_te_r]\n",
    "        X_te_r = None if X_te_all is None else X_te_all[idx_te_r, :]\n",
    "\n",
    "        try:\n",
    "            res_full = _fit_sarimax(y_tr_r, (X_tr_r if best_use_exog else None), best_order)\n",
    "            res_ext  = res_full.apply(endog=y_te_r, exog=(X_te_r if best_use_exog else None), refit=False)\n",
    "            yhat_r   = np.asarray(res_ext.fittedvalues, dtype=float)[-len(y_te_r):]\n",
    "            #DMSE-based BIC for the selected model on the training subseries\n",
    "            fit_resid = y_tr_r - np.asarray(res_full.fittedvalues, dtype=float)\n",
    "            ll_r = dmse_loglik(fit_resid, alpha=alpha)\n",
    "            k_r  = int(res_full.params.size)\n",
    "            n_r  = fit_resid.size\n",
    "            bic_r = -2.0 * ll_r + k_r * np.log(n_r)\n",
    "            bic_vals.append(bic_r)\n",
    "\n",
    "        except Exception:\n",
    "            yhat_r = np.full(shape=len(y_te_r), fill_value=float(np.mean(y_tr_r)), dtype=float)\n",
    "            bic_vals.append(np.nan)  #keep lengths aligned\n",
    "\n",
    "        #Place predictions back on the raw test rows for this remainder\n",
    "        yhat_te[idx_te_r] = yhat_r\n",
    "\n",
    "    #Ensure we produced all predictions (fill any remaining NaNs with subseries mean)\n",
    "    if np.isnan(yhat_te).any():\n",
    "        m = np.nanmean(yhat_te)\n",
    "        yhat_te = np.where(np.isnan(yhat_te), m, yhat_te)\n",
    "\n",
    "    diag = {\n",
    "    \"bic_mean\": float(np.nanmean(bic_vals)) if bic_vals else np.nan,\n",
    "    \"val_mse_mean\": float(np.nanmean(val_mse_vals)) if val_mse_vals else np.nan,\n",
    "    }\n",
    "    return yhat_te, df_te[\"Date\"].reset_index(drop=True), diag\n",
    "\n",
    "\n",
    "#Driver over all tickers\n",
    "def arima_subseries_pipeline(\n",
    "    df_train, df_test, predictors, H=10, alpha=9.0, val_frac=0.10, do_plots=False, tickers=None\n",
    "):\n",
    "    #If no list is provided, use all tickers present in BOTH train and test\n",
    "    if tickers is None:\n",
    "        all_train = pd.Index(df_train[\"Ticker\"].unique())\n",
    "        all_test  = pd.Index(df_test[\"Ticker\"].unique())\n",
    "        tickers = pd.Index(sorted(all_train.intersection(all_test)))\n",
    "    else:\n",
    "        #allow a single string or any iterable of tickers\n",
    "        if isinstance(tickers, str):\n",
    "            tickers = [tickers]\n",
    "        allowed = set(df_train[\"Ticker\"]).intersection(set(df_test[\"Ticker\"]))\n",
    "        tickers = pd.Index([t for t in tickers if t in allowed])\n",
    "\n",
    "    out_rows = []\n",
    "    preds_all = []\n",
    "\n",
    "    counter = 0\n",
    "    for tkr in tickers:\n",
    "        tr = df_train.loc[df_train[\"Ticker\"] == tkr].sort_values(\"Date\").reset_index(drop=True)\n",
    "        te = df_test.loc[df_test[\"Ticker\"] == tkr].sort_values(\"Date\").reset_index(drop=True)\n",
    "        if len(tr) == 0 or len(te) == 0:\n",
    "            continue\n",
    "\n",
    "        yhat_te, te_dates, diag = _predict_one_ticker(tr, te, predictors, H=H, alpha=alpha, val_frac=val_frac)\n",
    "        y_te = te[\"Target\"].to_numpy(dtype=float)\n",
    "        test_err = dmse_np(y_te, yhat_te, alpha=alpha)   #DMSE on test\n",
    "\n",
    "\n",
    "        #Store rows for global prediction DF\n",
    "        preds_all.append(pd.DataFrame({\n",
    "            \"Ticker\": tkr,\n",
    "            \"Date\": te_dates.to_numpy(),\n",
    "            \"TrueTarget\": y_te,\n",
    "            \"PredTarget_ARIMAX_subseries\": yhat_te\n",
    "        }))\n",
    "\n",
    "        out_rows.append({\n",
    "            \"Ticker\": tkr,\n",
    "            \"BIC\": diag[\"bic_mean\"],\n",
    "            \"Validation Error\": diag[\"val_mse_mean\"],  #MSE on validation\n",
    "            \"Test Error\": test_err                     #DMSE on test\n",
    "        })\n",
    "\n",
    "\n",
    "        if do_plots and counter < 10:\n",
    "            fig, ax = plt.subplots(figsize=(12, 4))\n",
    "            ax.plot(te_dates, y_te, lw=2.0, label=\"True Target\")\n",
    "            ax.plot(te_dates, yhat_te, lw=2.0, label=\"Predicted Target\")\n",
    "            ax.set_title(f\"{tkr}: Actual and Predicted Target, ARIMA/ARIMAX\",\n",
    "                         fontsize=20, weight=\"bold\")\n",
    "            ax.set_ylabel(\"Target\", fontsize=25)\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.tick_params(axis=\"x\", colors=\"black\", labelsize=15)\n",
    "            ax.tick_params(axis=\"y\", colors=\"black\", labelsize=18)\n",
    "            ax.legend(loc=\"upper right\", frameon=True, fontsize=20)\n",
    "            ax.set_facecolor(\"white\")\n",
    "            ax.grid(True, which=\"major\", linestyle=\"-\", alpha=0.15)\n",
    "            ax.spines[\"top\"].set_visible(False)\n",
    "            ax.spines[\"right\"].set_visible(False)\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "\n",
    "    preds_df = pd.concat(preds_all, axis=0, ignore_index=True)\n",
    "    summary_df = (\n",
    "        pd.DataFrame(out_rows)[[\"Ticker\", \"BIC\", \"Validation Error\", \"Test Error\"]]\n",
    "        .sort_values(\"Test Error\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    return preds_df, summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6cff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df_arimax, summary_df_arimax = arima_subseries_pipeline(\n",
    "    df_train, df_test, predictors, H=H, alpha=9.0, val_frac=0.10, do_plots=True)\n",
    "\n",
    "display(summary_df_arimax)\n",
    "\n",
    "df_test_with_arimax = (df_test.merge(preds_df_arimax[[\"Ticker\", \"Date\", \"PredTarget_ARIMAX_subseries\"]],\n",
    "                       on=[\"Ticker\", \"Date\"], how=\"left\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f8e7f",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEQ_LEN   = 20\n",
    "HIDDEN    = 64\n",
    "LAYERS    = 2  \n",
    "BATCH_T   = 32\n",
    "LR        = 3e-3\n",
    "EPOCHS    = 40\n",
    "PATIENCE  = 10\n",
    "ALPHA     = loss_penalty\n",
    "EPS       = 1e-8\n",
    "DROPOUT   = 0.20         \n",
    "BIDIR     = True         \n",
    "HEAD_H    = 128   \n",
    "\n",
    "def dmse_loss(y_true, y_pred, alpha=loss_penalty, reduction=\"mean\"):\n",
    "    \"\"\"\n",
    "    Numpy/Pandas-friendly wrapper around the DMSE torch loss.\n",
    "    Returns a Python float (mean by default).\n",
    "    \"\"\"\n",
    "    #to 1-D torch tensors\n",
    "    y_true_t = torch.as_tensor(np.asarray(y_true).ravel(), dtype=torch.float32)\n",
    "    y_pred_t = torch.as_tensor(np.asarray(y_pred).ravel(), dtype=torch.float32)\n",
    "\n",
    "    #mask out NaN/inf just in case\n",
    "    m = torch.isfinite(y_true_t) & torch.isfinite(y_pred_t)\n",
    "    if not torch.any(m):\n",
    "        return float(\"nan\")\n",
    "\n",
    "    crit = DMSE(over_penalty=alpha, reduction=reduction)\n",
    "    with torch.no_grad():\n",
    "        val = crit(y_hat=y_pred_t[m], y=y_true_t[m]).item()\n",
    "    return float(val)\n",
    "\n",
    "def _ensure_dt_naive(df_like):\n",
    "    df_like[\"Date\"] = pd.to_datetime(df_like[\"Date\"]).dt.tz_localize(None)\n",
    "    return df_like\n",
    "\n",
    "for _d in (df2, df_train, df_test):\n",
    "    _ensure_dt_naive(_d)\n",
    "\n",
    "ticker_to_idx = {t:i for i,t in enumerate(tickers)}\n",
    "\n",
    "#Time ranges (inclusive)\n",
    "train_start = pd.to_datetime(df_train[\"Date\"]).min()\n",
    "train_end   = pd.to_datetime(df_train[\"Date\"]).max()\n",
    "test_start  = pd.to_datetime(df_test[\"Date\"]).min()\n",
    "test_end    = pd.to_datetime(df_test[\"Date\"]).max()\n",
    "\n",
    "F = len(predictors)\n",
    "S = len(tickers)\n",
    "\n",
    "#Build aligned panel tensors: X_panel[T, S, F], y_panel[T, S], dates[T]\n",
    "def build_panel_arrays(df, tickers, predictors):\n",
    "    df = df.sort_values([\"Date\", \"Ticker\"])\n",
    "    dates = pd.to_datetime(sorted(df[\"Date\"].unique()))\n",
    "    T = len(dates)\n",
    "    X_panel = np.empty((T, len(tickers), len(predictors)), dtype=np.float32)\n",
    "    y_panel = np.empty((T, len(tickers)), dtype=np.float32)\n",
    "\n",
    "    #pre-index by date for quick reindex per ticker\n",
    "    for s, tkr in enumerate(tickers):\n",
    "        sub = (df.loc[df[\"Ticker\"] == tkr]\n",
    "                 .set_index(\"Date\")\n",
    "                 .reindex(dates))  #ensures alignment\n",
    "        X_panel[:, s, :] = sub[predictors].to_numpy(dtype=np.float32)\n",
    "        y_panel[:, s]    = sub[\"Target\"].to_numpy(dtype=np.float32)\n",
    "    return dates, X_panel, y_panel\n",
    "\n",
    "dates_all, X_panel, y_panel = build_panel_arrays(df2, tickers, predictors)\n",
    "T_total = len(dates_all)\n",
    "\n",
    "#Observability mask: Target must be known for all tickers at time t\n",
    "obs_mask_all = np.all(~np.isnan(y_panel), axis=1)\n",
    "\n",
    "#Time index helpers\n",
    "train_time_mask = (dates_all >= train_start) & (dates_all <= train_end)\n",
    "test_time_mask  = (dates_all >= test_start)  & (dates_all <= test_end)\n",
    "\n",
    "#For sequence t, we need indices [t-SEQ_LEN, ..., t-1] available.\n",
    "seq_ok_mask = np.arange(T_total) >= SEQ_LEN\n",
    "\n",
    "#Standardise predictors using ONLY training history that can appear in input windows\n",
    "scaler_time_mask = (dates_all <= train_end)\n",
    "X_mu = np.nanmean(X_panel[scaler_time_mask], axis=(0,1))    \n",
    "X_sd = np.nanstd (X_panel[scaler_time_mask], axis=(0,1)) + EPS\n",
    "X_panel = (X_panel - X_mu) / X_sd\n",
    "\n",
    "#Custom class for the dataset\n",
    "class PanelSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Items are time indices t. Each item returns:\n",
    "      X: (S, SEQ_LEN, F)\n",
    "      y: (S,)\n",
    "      t: int  (time index)\n",
    "    \"\"\"\n",
    "    def __init__(self, X_panel, y_panel, dates, time_mask):\n",
    "        self.X = X_panel\n",
    "        self.y = y_panel\n",
    "        self.dates = dates\n",
    "        base = np.where(time_mask & obs_mask_all & seq_ok_mask)[0]\n",
    "        self.t_idx = base.astype(np.int32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.t_idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        t = int(self.t_idx[i])\n",
    "        X_seq = self.X[t-SEQ_LEN:t, :, :]           #(SEQ_LEN, S, F)\n",
    "        X_seq = np.transpose(X_seq, (1,0,2))        #(S, SEQ_LEN, F)\n",
    "        y_t   = self.y[t, :]                        #(S,)\n",
    "        return torch.from_numpy(X_seq), torch.from_numpy(y_t), t  #this is an integer\n",
    "\n",
    "#Build time masks for train/val/test\n",
    "#Validation = last 10% of training time indices (after all filters)\n",
    "train_ds_tmp = PanelSequenceDataset(X_panel, y_panel, dates_all, train_time_mask)\n",
    "n_train_times = len(train_ds_tmp)\n",
    "n_val_times   = max(1, int(0.10 * n_train_times))\n",
    "#Split by raw order:\n",
    "train_time_idx = train_ds_tmp.t_idx[:-n_val_times] if n_train_times > n_val_times else train_ds_tmp.t_idx[:0]\n",
    "val_time_idx   = train_ds_tmp.t_idx[-n_val_times:] if n_val_times > 0 else np.array([], dtype=np.int32)\n",
    "\n",
    "def mask_from_indices(idxs, T_len):\n",
    "    m = np.zeros(T_len, dtype=bool); m[idxs] = True; return m\n",
    "\n",
    "train_mask_final = mask_from_indices(train_time_idx, T_total)\n",
    "val_mask_final   = mask_from_indices(val_time_idx,   T_total)\n",
    "\n",
    "train_ds = PanelSequenceDataset(X_panel, y_panel, dates_all, train_mask_final)\n",
    "val_ds   = PanelSequenceDataset(X_panel, y_panel, dates_all, val_mask_final)\n",
    "test_ds  = PanelSequenceDataset(X_panel, y_panel, dates_all, test_time_mask)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_T, shuffle=True,  drop_last=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_T, shuffle=False, drop_last=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_T, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"[LSTM] Tickers: {S}, Features: {F}, Train steps: {len(train_ds)}, Val steps: {len(val_ds)}, Test steps: {len(test_ds)}\")\n",
    "\n",
    "#Target standardisation (per-ticker, only on the training set)\n",
    "train_targets_mat = y_panel[train_ds.t_idx, :]                              #(n_train_times, S)\n",
    "Y_MU = np.nanmean(train_targets_mat, axis=0).astype(np.float32)             #(S,)\n",
    "Y_SD = (np.nanstd(train_targets_mat, axis=0) + EPS).astype(np.float32)      #(S,)\n",
    "\n",
    "Y_MU_T = torch.tensor(Y_MU, dtype=torch.float32, device=device)             #(S,)\n",
    "Y_SD_T = torch.tensor(Y_SD, dtype=torch.float32, device=device)             #(S,)\n",
    "\n",
    "print(f\"[LSTM] y mean (median across tickers)={np.nanmedian(Y_MU):.5f}, \"\n",
    "      f\"y std (median across tickers)={np.nanmedian(Y_SD):.5f}\")\n",
    "\n",
    "#Custom model class\n",
    "class SharedTickerLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared per-ticker BiLSTM encoder + attention pooling + MLP head (non-negative output).\n",
    "    Forward expects X of shape (B, S, SEQ_LEN, F); we flatten to (B*S, SEQ_LEN, F).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size=HIDDEN, num_layers=LAYERS,\n",
    "                 bidirectional=BIDIR, dropout=DROPOUT, head_hidden=HEAD_H):\n",
    "        super().__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.D = 2 if bidirectional else 1\n",
    "\n",
    "        #Normalise features step-wise before the LSTM\n",
    "        self.in_norm = nn.LayerNorm(input_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=(dropout if num_layers > 1 else 0.0),\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        #Additive attention over the SEQ_LEN dimension\n",
    "        self.attn = nn.Linear(hidden_size * self.D, 1, bias=False)\n",
    "\n",
    "        #Head now ends with Softplus to keep y_hat >= 0 during training\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * self.D, head_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.10),\n",
    "            nn.Linear(head_hidden, 1),\n",
    "            nn.Softplus() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):  #x: (B, S, SEQ_LEN, F)\n",
    "        B, S, L, F_ = x.shape\n",
    "        x = x.reshape(B*S, L, F_)             #(B*S, SEQ_LEN, F)\n",
    "        x = self.in_norm(x)                   \n",
    "        out, _ = self.lstm(x)                 #(B*S, SEQ_LEN, H*D)\n",
    "\n",
    "        scores = self.attn(out).squeeze(-1)   #(B*S, SEQ_LEN)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = (out * weights.unsqueeze(-1)).sum(dim=1)\n",
    "\n",
    "        y_hat = self.head(context).squeeze(-1)  #(B*S,)\n",
    "        return y_hat.view(B, S)\n",
    "\n",
    "\n",
    "model = SharedTickerLSTM(input_size=F, hidden_size=HIDDEN, num_layers=LAYERS).to(device)\n",
    "with torch.no_grad():\n",
    "    #small Xavier on the first head layer is fine by default, set last layer close to mean\n",
    "    last_linear = model.head[-2]   #the final Linear before Softplus\n",
    "    last_linear.bias.fill_(float(np.nanmean(Y_MU)))\n",
    "    last_linear.weight.mul_(0.01)\n",
    "\n",
    "criterion = DMSE(over_penalty=ALPHA, reduction=\"mean\").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=2, min_lr=1e-5, verbose=True\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"[LSTM] Trainable parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dmse(loader):\n",
    "    model.eval()\n",
    "    tot_loss, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb, _ in loader:\n",
    "            Xb = Xb.to(device, dtype=torch.float32)\n",
    "            yb = yb.to(device, dtype=torch.float32) \n",
    "            yhat = model(Xb) \n",
    "            loss = criterion((yhat - Y_MU_T)/Y_SD_T, (yb - Y_MU_T)/Y_SD_T)\n",
    "            bs = yb.numel()\n",
    "            tot_loss += float(loss.item()) * bs\n",
    "            n += bs\n",
    "    return tot_loss / max(1, n)\n",
    "\n",
    "best_val = np.inf\n",
    "best_state = None\n",
    "train_curve, val_curve = [], []\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    for Xb, yb, _ in train_loader:\n",
    "        Xb = Xb.to(device, dtype=torch.float32)\n",
    "        yb = yb.to(device, dtype=torch.float32)\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(Xb)\n",
    "        loss = criterion((yhat - Y_MU_T)/Y_SD_T, (yb - Y_MU_T)/Y_SD_T)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    tr = eval_dmse(train_loader)\n",
    "    va = eval_dmse(val_loader)\n",
    "    train_curve.append(tr); val_curve.append(va)\n",
    "\n",
    "    if va + 1e-10 < best_val:\n",
    "        best_val = va\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    print(f\"[Epoch {epoch:02d}] train DMSE={tr:.5f} | val DMSE={va:.5f} | best={best_val:.5f}\")\n",
    "    scheduler.step(va)\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping (patience={PATIENCE})\")\n",
    "        break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192d5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_records = []\n",
    "with torch.no_grad():\n",
    "    for Xb, yb, tb in test_loader:          #tb is (B,) tensor of int time indices\n",
    "        Xb = Xb.to(device, dtype=torch.float32)\n",
    "        yb = yb.to(device, dtype=torch.float32)\n",
    "        yhat = model(Xb)\n",
    "        y_true = yb.cpu().numpy()\n",
    "        y_pred = np.maximum(yhat.cpu().numpy(), 0.0)\n",
    "        tb = tb.cpu().numpy().astype(int)  \n",
    "        for b in range(y_true.shape[0]):\n",
    "            dt = dates_all[int(tb[b])]    \n",
    "            for s, tkr in enumerate(tickers):\n",
    "                test_records.append((dt, tkr, float(y_true[b, s]), float(y_pred[b, s])))\n",
    "\n",
    "lstm_pred_df = (pd.DataFrame(test_records, columns=[\"Date\",\"Ticker\",\"True\",\"Pred\"])\n",
    "                  .sort_values([\"Ticker\",\"Date\"])\n",
    "                  .reset_index(drop=True))\n",
    "\n",
    "for tkr in tickers[10:20]:\n",
    "    sub = lstm_pred_df.loc[lstm_pred_df[\"Ticker\"] == tkr].sort_values(\"Date\")\n",
    "    if sub.empty:\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(sub[\"Date\"], sub[\"True\"], lw=2.0, label=\"True Target\")\n",
    "    ax.plot(sub[\"Date\"], sub[\"Pred\"], lw=2.0, label=\"Predicted Target\")\n",
    "\n",
    "    ax.set_title(f\"{tkr}: Actual and Predicted Target, LSTM NN Trained on all Stocks\",\n",
    "                 fontsize=20, weight=\"bold\")\n",
    "    ax.set_ylabel(\"Target\", fontsize=25)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.tick_params(axis=\"x\", colors=\"black\", labelsize=15)\n",
    "    ax.tick_params(axis=\"y\", colors=\"black\", labelsize=18)\n",
    "    ax.legend(loc=\"upper right\", frameon=True, fontsize=20)\n",
    "    ax.set_facecolor(\"white\")\n",
    "    ax.grid(True, which=\"major\", linestyle=\"-\", alpha=0.15)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "#Per-ticker DMSE (test) and validation DMSE (computed by re-running eval on val set by ticker)\n",
    "def per_ticker_dmse(df):\n",
    "    out = []\n",
    "    for tkr, g in df.groupby(\"Ticker\"):\n",
    "        out.append((tkr, dmse_loss(g[\"True\"].to_numpy(), g[\"Pred\"].to_numpy(), alpha=ALPHA)))\n",
    "    return pd.DataFrame(out, columns=[\"Ticker\",\"Test_DMSE\"])\n",
    "\n",
    "test_dmse_table = per_ticker_dmse(lstm_pred_df)\n",
    "\n",
    "#Validation per ticker: pass back through val_loader and collect predictions similarly\n",
    "val_records = []\n",
    "with torch.no_grad():\n",
    "    for Xb, yb, tb in val_loader:\n",
    "        Xb = Xb.to(device, dtype=torch.float32)\n",
    "        yb = yb.to(device, dtype=torch.float32)\n",
    "        yhat = model(Xb)\n",
    "        y_true = yb.cpu().numpy()\n",
    "        y_pred = np.maximum(yhat.cpu().numpy(), 0.0)\n",
    "        tb = tb.cpu().numpy().astype(int)\n",
    "        for b in range(y_true.shape[0]):\n",
    "            dt = dates_all[int(tb[b])]\n",
    "            for s, tkr in enumerate(tickers):\n",
    "                val_records.append((dt, tkr, float(y_true[b, s]), float(y_pred[b, s])))\n",
    "\n",
    "lstm_val_df = (pd.DataFrame(val_records, columns=[\"Date\",\"Ticker\",\"True\",\"Pred\"])\n",
    "                 .sort_values([\"Ticker\",\"Date\"])\n",
    "                 .reset_index(drop=True))\n",
    "\n",
    "overall_val_dmse = dmse_loss(\n",
    "    lstm_val_df[\"True\"].to_numpy(),\n",
    "    lstm_val_df[\"Pred\"].to_numpy(),\n",
    "    alpha=ALPHA)\n",
    "\n",
    "overall_test_dmse = dmse_loss(\n",
    "    lstm_pred_df[\"True\"].to_numpy(),\n",
    "    lstm_pred_df[\"Pred\"].to_numpy(),\n",
    "    alpha=ALPHA)\n",
    "\n",
    "print(f\"LSTM NN Overall Test DMSE: {overall_test_dmse:.5f}\")\n",
    "print(f\"LSTM NN Overall Validation DMSE: {overall_val_dmse:.5f}\")\n",
    "\n",
    "val_dmse_table = per_ticker_dmse(lstm_val_df).rename(columns={\"Test_DMSE\":\"Val_DMSE\"})\n",
    "\n",
    "#Merge and sort\n",
    "lstm_results_table = (val_dmse_table.merge(test_dmse_table, on=\"Ticker\", how=\"outer\")\n",
    "                      .sort_values(\"Test_DMSE\")\n",
    "                      .reset_index(drop=True))\n",
    "\n",
    "display_cols = [\"Ticker\",\"Val_DMSE\",\"Test_DMSE\"]\n",
    "display(lstm_results_table[display_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32349cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTickerSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Sequences for one ticker s_idx.\n",
    "    Returns:\n",
    "      X: (1, SEQ_LEN, F)   y: (1,)   t: int time index (use dates_all[t] to map)\n",
    "    \"\"\"\n",
    "    def __init__(self, X_panel, y_panel, dates, time_mask, s_idx):\n",
    "        self.Xs = X_panel[:, s_idx, :]           #(T, F)\n",
    "        self.ys = y_panel[:, s_idx]              #(T,)\n",
    "        self.dates = dates\n",
    "        obs_mask_one = ~np.isnan(self.ys)\n",
    "        base = np.where(time_mask & obs_mask_one & seq_ok_mask)[0]\n",
    "        self.t_idx = base.astype(np.int32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.t_idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        t = int(self.t_idx[i])\n",
    "        X_seq = self.Xs[t-SEQ_LEN:t, :].astype(np.float32)   #(SEQ_LEN, F)\n",
    "        X_seq = X_seq.reshape(1, SEQ_LEN, X_seq.shape[-1])   #add S=1 dim\n",
    "        y_t   = np.array([self.ys[t]], dtype=np.float32)     #(1,)\n",
    "        return torch.from_numpy(X_seq), torch.from_numpy(y_t), t\n",
    "\n",
    "#Train a single model for each ticker\n",
    "def train_single_ticker_model(s_idx, ticker):\n",
    "    torch.manual_seed(0)\n",
    "    train_ds_st = SingleTickerSequenceDataset(X_panel, y_panel, dates_all, train_mask_final, s_idx)\n",
    "    val_ds_st   = SingleTickerSequenceDataset(X_panel, y_panel, dates_all, val_mask_final,   s_idx)\n",
    "    test_ds_st  = SingleTickerSequenceDataset(X_panel, y_panel, dates_all, test_time_mask,  s_idx)\n",
    "\n",
    "    train_loader_st = DataLoader(train_ds_st, batch_size=BATCH_T, shuffle=True,  drop_last=False)\n",
    "    val_loader_st   = DataLoader(val_ds_st,   batch_size=BATCH_T, shuffle=False, drop_last=False)\n",
    "    test_loader_st  = DataLoader(test_ds_st,  batch_size=BATCH_T, shuffle=False, drop_last=False)\n",
    "\n",
    "    model_st = SharedTickerLSTM(input_size=F, hidden_size=HIDDEN, num_layers=LAYERS).to(device)\n",
    "    with torch.no_grad():\n",
    "        last_linear = model_st.head[-2]\n",
    "        init_bias = Y_MU_T[s_idx] if getattr(Y_MU_T, \"ndim\", 0) > 0 else Y_MU_T\n",
    "        last_linear.bias.data.fill_(init_bias.item())\n",
    "        last_linear.weight.data.mul_(0.01)\n",
    "\n",
    "    criterion_st = DMSE(over_penalty=ALPHA, reduction=\"mean\").to(device)\n",
    "    optimizer_st = torch.optim.AdamW(model_st.parameters(), lr=LR, weight_decay=1e-3)\n",
    "    scheduler_st = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_st, mode=\"min\", factor=0.5, patience=2, min_lr=1e-5, verbose=False\n",
    "    )\n",
    "\n",
    "    def _eval(loader):\n",
    "        model_st.eval()\n",
    "        tot, n = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb, _ in loader:\n",
    "                Xb = Xb.to(device, dtype=torch.float32)   #(B,1,L,F)\n",
    "                yb = yb.to(device, dtype=torch.float32)   #(B,1)\n",
    "                yhat = model_st(Xb)                       #(B,1)\n",
    "                loss = criterion_st((yhat - Y_MU_T)/Y_SD_T, (yb - Y_MU_T)/Y_SD_T)\n",
    "                bs = yb.numel()\n",
    "                tot += float(loss.item()) * bs\n",
    "                n += bs\n",
    "        return tot / max(1, n)\n",
    "\n",
    "    best_val, best_state, no_improve = np.inf, None, 0\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model_st.train()\n",
    "        for Xb, yb, _ in train_loader_st:\n",
    "            Xb = Xb.to(device, dtype=torch.float32)\n",
    "            yb = yb.to(device, dtype=torch.float32)\n",
    "            optimizer_st.zero_grad()\n",
    "            yhat = model_st(Xb)\n",
    "            loss = criterion_st((yhat - Y_MU_T)/Y_SD_T, (yb - Y_MU_T)/Y_SD_T)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model_st.parameters(), max_norm=5.0)\n",
    "            optimizer_st.step()\n",
    "\n",
    "        va = _eval(val_loader_st)\n",
    "        scheduler_st.step(va)\n",
    "        if va + 1e-10 < best_val:\n",
    "            best_val = va\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model_st.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model_st.load_state_dict(best_state)\n",
    "\n",
    "    #Test predictions\n",
    "    preds, trues, dts = [], [], []\n",
    "    model_st.eval()\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb, tb in test_loader_st:\n",
    "            Xb = Xb.to(device, dtype=torch.float32)\n",
    "            yb = yb.to(device, dtype=torch.float32)\n",
    "            yhat = model_st(Xb).clamp_min_(0.0) \n",
    "            y_true = yb.cpu().numpy()\n",
    "            y_pred = yhat.cpu().numpy()\n",
    "            tb = tb.cpu().numpy().astype(int)\n",
    "            for b in range(y_true.shape[0]):\n",
    "                dts.append(dates_all[int(tb[b])])\n",
    "                trues.append(float(y_true[b, 0]))\n",
    "                preds.append(float(y_pred[b, 0]))\n",
    "\n",
    "    pred_df = (pd.DataFrame({\"Date\": dts, \"Ticker\": ticker, \"True\": trues, \"Pred\": preds})\n",
    "                 .sort_values(\"Date\").reset_index(drop=True))\n",
    "    test_dmse = dmse_loss(pred_df[\"True\"].to_numpy(), pred_df[\"Pred\"].to_numpy(), alpha=ALPHA)\n",
    "    return {\"val_dmse\": float(best_val), \"test_dmse\": float(test_dmse)}, pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92482f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we produce the data in Table 6\n",
    "\n",
    "single_results = []\n",
    "single_preds_by_ticker = {}\n",
    "\n",
    "for s, tkr in enumerate(tickers):\n",
    "    stats, pred_df = train_single_ticker_model(s, tkr)\n",
    "    single_results.append({\"Ticker\": tkr, \"Val_DMSE\": stats[\"val_dmse\"], \"Test_DMSE\": stats[\"test_dmse\"]})\n",
    "    single_preds_by_ticker[tkr] = pred_df\n",
    "\n",
    "lstm_single_results_table = pd.DataFrame(single_results).sort_values(\"Test_DMSE\").reset_index(drop=True)\n",
    "display(lstm_single_results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f797e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates the plots in Figure 18\n",
    "\n",
    "for tkr in tickers[10:20]:\n",
    "    g = single_preds_by_ticker.get(tkr)\n",
    "    if g is None or g.empty:\n",
    "        continue\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(g[\"Date\"], g[\"True\"], lw=2.0, label=\"True Target\")\n",
    "    ax.plot(g[\"Date\"], g[\"Pred\"], lw=2.0, label=\"Predicted Target\")\n",
    "    ax.set_title(f\"{tkr}: Actual and Predicted Target, Per-Ticker LSTM NN\",\n",
    "                 fontsize=20, weight=\"bold\")\n",
    "    ax.set_ylabel(\"Target\", fontsize=25)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.tick_params(axis=\"x\", colors=\"black\", labelsize=15)\n",
    "    ax.tick_params(axis=\"y\", colors=\"black\", labelsize=18)\n",
    "    ax.legend(loc=\"upper right\", frameon=True, fontsize=20)\n",
    "    ax.set_facecolor(\"white\"); ax.grid(True, which=\"major\", linestyle=\"-\", alpha=0.15)\n",
    "    ax.spines[\"top\"].set_visible(False); ax.spines[\"right\"].set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94534f53",
   "metadata": {},
   "source": [
    "# Backtesting a Trading Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985ca1e",
   "metadata": {},
   "source": [
    "### Creating the Dataset for Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3d0a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the dataset that will be used for backtesting. It contains data for the years 2022, 2023, 2024\n",
    "\n",
    "#We take data from \"2021-11-01\" to avoid the first days of 2022 being null. We will later drop 2021 data\n",
    "spy_return = (\n",
    "    yf.Ticker(\"SPY\")\n",
    "      .history(start=\"2021-11-01\", end=end_date_backtest, auto_adjust=False)[\"Close\"]\n",
    "      .pct_change()\n",
    ")\n",
    "\n",
    "spy_return.index = spy_return.index.tz_localize(None)\n",
    "\n",
    "bt_frames = []\n",
    "for tkr in tickers:\n",
    "    try:\n",
    "        tmp = (\n",
    "            yf.Ticker(tkr)\n",
    "              .history(start=\"2021-11-01\", end=end_date_backtest, auto_adjust=False)\n",
    "              .reset_index()\n",
    "              .assign(Ticker=tkr, Sector=sector_of.get(tkr, \"Unknown\"))\n",
    "              .loc[:, [\"Date\", \"Ticker\", \"Sector\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "        )\n",
    "        bt_frames.append(tmp)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {tkr} (price fetch failed): {e}\")\n",
    "\n",
    "df_backtest = pd.concat(bt_frames, ignore_index=True)\n",
    "\n",
    "#Normalise date and sector code\n",
    "df_backtest[\"Date\"] = pd.to_datetime(df_backtest[\"Date\"]).dt.date\n",
    "df_backtest[\"Sector\"] = df_backtest[\"Sector\"].map(sector_dict).fillna(0).astype(int)\n",
    "\n",
    "#Compute features and lagged targets only (not the target variable)\n",
    "df_backtest = (\n",
    "    df_backtest\n",
    "      .groupby(\"Ticker\", group_keys=False, sort=False).apply(add_features)\n",
    "      .groupby(\"Ticker\", group_keys=False, sort=False).apply(add_targets, target_horizons=horizons, future=False)\n",
    ")\n",
    "\n",
    "_cut_date = pd.to_datetime(end_date_analysis).date()\n",
    "df_backtest = df_backtest[df_backtest[\"Date\"] >= _cut_date].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef847fe9",
   "metadata": {},
   "source": [
    "### Estimating the Straddle Premia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a06c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest[\"Date\"] = pd.to_datetime(df_backtest[\"Date\"]).dt.date\n",
    "\n",
    "#Build \"risk_free_df\" from ^IRX (13-week T-bill)\n",
    "start_dt = pd.to_datetime(df_backtest[\"Date\"]).min() - pd.Timedelta(days=10)\n",
    "end_dt   = pd.to_datetime(df_backtest[\"Date\"]).max() + pd.Timedelta(days=10)\n",
    "\n",
    "irx = yf.download(\"^IRX\", start=start_dt, end=end_dt, progress=False, auto_adjust=False)\n",
    "\n",
    "if isinstance(irx.columns, pd.MultiIndex):\n",
    "    irx.columns = irx.columns.get_level_values(0)\n",
    "\n",
    "#Take the close price, reset the index, and standardise the column names\n",
    "risk_free_df = irx[[\"Close\"]].copy().reset_index()\n",
    "risk_free_df.rename(\n",
    "    columns={risk_free_df.columns[0]: \"Date\", \"Close\": \"irx_discount_pct\"},\n",
    "    inplace=True\n",
    ")\n",
    "risk_free_df[\"Date\"] = pd.to_datetime(risk_free_df[\"Date\"]).dt.date\n",
    "\n",
    "#Convert bank-discount yield (percent) to bond-equivalent annual simple yield\n",
    "d = (risk_free_df[\"irx_discount_pct\"] / 100.0).astype(float)     \n",
    "price = 1.0 - d * (91.0 / 360.0) \n",
    "risk_free_df[\"rf_annual_simple\"] = ((1.0 / price - 1.0) * (365.0 / 91.0)).astype(float)\n",
    "\n",
    "#Keep one row per calendar date\n",
    "risk_free_df = (\n",
    "    risk_free_df[[\"Date\", \"rf_annual_simple\"]]\n",
    "    .dropna(subset=[\"rf_annual_simple\"])\n",
    "    .drop_duplicates(subset=[\"Date\"], keep=\"last\")\n",
    ")\n",
    "\n",
    "#Merge onto backtest frame\n",
    "df_backtest = df_backtest.merge(risk_free_df, on=\"Date\", how=\"left\", suffixes=(\"\", \"_rf\"))\n",
    "\n",
    "#Normalise to a single column name 'rf_annual_simple'\n",
    "if \"rf_annual_simple_rf\" in df_backtest.columns and \"rf_annual_simple\" in df_backtest.columns:\n",
    "    df_backtest[\"rf_annual_simple\"] = df_backtest[\"rf_annual_simple\"].fillna(df_backtest[\"rf_annual_simple_rf\"])\n",
    "    df_backtest.drop(columns=[\"rf_annual_simple_rf\"], inplace=True)\n",
    "elif \"rf_annual_simple_rf\" in df_backtest.columns and \"rf_annual_simple\" not in df_backtest.columns:\n",
    "    df_backtest.rename(columns={\"rf_annual_simple_rf\": \"rf_annual_simple\"}, inplace=True)\n",
    "\n",
    "#Forward/back-fill gaps. If still missing at the very start, default to 0\n",
    "df_backtest[\"rf_annual_simple\"] = (\n",
    "    df_backtest[\"rf_annual_simple\"].astype(float)\n",
    "    .fillna(method=\"ffill\")\n",
    "    .fillna(method=\"bfill\")\n",
    "    .fillna(0.0)\n",
    ")\n",
    "\n",
    "#Continuous-compounded risk-free rate for Black–Scholes\n",
    "df_backtest[\"rf_cc\"] = np.log1p(df_backtest[\"rf_annual_simple\"])\n",
    "\n",
    "#Annualised volatility\n",
    "df_backtest[\"sigma_annual\"] = (df_backtest[\"Volatility\"].astype(float) * np.sqrt(252)).clip(lower=1e-6)\n",
    "\n",
    "#Black–Scholes ATM (K = S) 10-day straddle\n",
    "T_years = 10.0 / 365.0\n",
    "\n",
    "S = df_backtest[\"Close\"].astype(float).to_numpy()\n",
    "r = df_backtest[\"rf_cc\"].astype(float).to_numpy()\n",
    "sigma = df_backtest[\"sigma_annual\"].astype(float).to_numpy()\n",
    "\n",
    "sig_sqrtT = np.maximum(sigma * np.sqrt(T_years), 1e-12)  #numerical guard\n",
    "d1 = ((r + 0.5 * sigma**2) * T_years) / sig_sqrtT\n",
    "d2 = d1 - sig_sqrtT\n",
    "\n",
    "Nd1 = ndtr(d1)\n",
    "Nd2 = ndtr(d2)\n",
    "disc = np.exp(-r * T_years)\n",
    "\n",
    "calls = S * Nd1 - S * disc * Nd2\n",
    "puts  = S * disc * (1.0 - Nd2) - S * (1.0 - Nd1)\n",
    "\n",
    "df_backtest[\"Straddle_premium\"] = calls + puts\n",
    "\n",
    "#Clean up helper columns\n",
    "df_backtest.drop(columns=[\"rf_annual_simple\", \"rf_cc\", \"sigma_annual\"], inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582367eb",
   "metadata": {},
   "source": [
    "### Forecasting \"Target\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d62f48",
   "metadata": {},
   "source": [
    "##### Preliminary Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8642437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encode the \"Sector\" variable\n",
    "sector_dummies_backtest = pd.get_dummies(df_backtest[\"Sector\"], prefix=\"Sector\", dtype=int, drop_first=True)\n",
    "df_backtest = pd.concat([df_backtest, sector_dummies_backtest], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per-ticker mean and std for every predictor, computed on df2\n",
    "scaler_dict_backtest = {}\n",
    "for ticker, g in df2.groupby(\"Ticker\"):\n",
    "    mu = g[continuous_predictors].mean().astype(\"float32\")\n",
    "    std = g[continuous_predictors].std(ddof=0).replace(0, 1e-12).astype(\"float32\")  #avoid /0\n",
    "    scaler_dict_backtest[ticker] = {\"mean\": mu, \"std\": std}\n",
    "\n",
    "#helper that standardises using the stats stored in scaler_dict_backtest\n",
    "def standardise_df2(df_part: pd.DataFrame, variables: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a copy of df_part where the columns in *variables* are\n",
    "    standardised (per-ticker) using the mean / std computed on df2.\n",
    "    All other columns are left unchanged.\n",
    "    \"\"\"\n",
    "    pieces = []\n",
    "    for ticker, g in df_part.groupby(\"Ticker\", sort=False):\n",
    "        m = scaler_dict_backtest[ticker][\"mean\"]\n",
    "        s = scaler_dict_backtest[ticker][\"std\"]\n",
    "        g_std = g.copy()\n",
    "        g_std[variables] = ((g_std[variables] - m) / s).astype(\"float32\")\n",
    "        pieces.append(g_std)\n",
    "\n",
    "    #Re-assemble in original row order\n",
    "    df_out = pd.concat(pieces).loc[df_part.index]\n",
    "    return df_out\n",
    "\n",
    "#Standardise df2 and df_backtest using df2 stats\n",
    "df2_stand = standardise_df2(df2, continuous_predictors)\n",
    "df_backtest_stand = standardise_df2(df_backtest, continuous_predictors)\n",
    "\n",
    "X_df2, X_df2_stand, y_df2 = df2[predictors], df2_stand[predictors], df2[\"Target\"]\n",
    "X_backtest, X_backtest_stand = df_backtest[predictors], df_backtest_stand[predictors]\n",
    "\n",
    "#Cast to NumPy float32 (and 2D targets)\n",
    "X_df2_np = X_df2.values.astype(\"float32\")\n",
    "X_df2_stand_np = X_df2_stand.values.astype(\"float32\")\n",
    "\n",
    "X_backtest_np = X_backtest.values.astype(\"float32\")\n",
    "X_backtest_stand_np = X_backtest_stand.values.astype(\"float32\")\n",
    "\n",
    "y_df2_np = y_df2.values.astype(\"float32\").reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ac0c4c",
   "metadata": {},
   "source": [
    "##### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df2_lr = df2_stand[predictors_lr].values.astype(\"float32\")\n",
    "X_backtest_lr = df_backtest_stand[predictors_lr].values.astype(\"float32\")\n",
    "\n",
    "y_df2_col = y_df2_np if getattr(y_df2_np, \"ndim\", 1) == 2 else y_df2_np.reshape(-1, 1)\n",
    "\n",
    "backtest_lr_model = train_model(\n",
    "    \"linear regression\",\n",
    "    X_df2_lr,\n",
    "    y_df2_col,\n",
    "    epochs=15,\n",
    "    learn_rate=5e-2,\n",
    "    batch_size=len(X_df2_lr)\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    device = next(backtest_lr_model.parameters()).device\n",
    "    X_bt_t = torch.as_tensor(X_backtest_lr, dtype=torch.float32, device=device)\n",
    "    preds_bt = backtest_lr_model(X_bt_t)\n",
    "    preds_bt = torch.clamp(preds_bt, min=0.0)   \n",
    "    preds_bt_np = preds_bt.squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "df_backtest.loc[:, \"Predictions_lr\"] = preds_bt_np.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe40e72",
   "metadata": {},
   "source": [
    "##### Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd6a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "try:\n",
    "    ticker_ids_df2 = df2_stand[\"Ticker\"].to_numpy()\n",
    "except Exception:\n",
    "    try:\n",
    "        ticker_ids_df2 = df2[\"Ticker\"].to_numpy()\n",
    "    except Exception:\n",
    "        try:\n",
    "            ticker_ids_df2 = df_train[\"Ticker\"].to_numpy()\n",
    "        except Exception:\n",
    "            ticker_ids_df2 = None  \n",
    "\n",
    "configs = [\n",
    "    (\"SpectralNorm+Dropout\", dict(dropout=0.20, batchnorm=False, spectral_norm=True,\n",
    "                                  input_noise_std=0.0, lr=1e-3, weight_decay=5e-4,\n",
    "                                  l1=0.0, l2=0.0, hidden_sizes=(512,256,128,64), patience=15, epochs=80)),\n",
    "    (\"ElasticNet+Dropout\",   dict(dropout=0.30, batchnorm=False, spectral_norm=False,\n",
    "                                  input_noise_std=0.0, lr=1e-3, weight_decay=0.0,\n",
    "                                  l1=1e-6, l2=1e-5, hidden_sizes=(512,256,128,64), patience=15, epochs=80)),\n",
    "    (\"L2+Dropout+BatchNorm\", dict(dropout=0.25, batchnorm=True,  spectral_norm=False,\n",
    "                                  input_noise_std=0.0, lr=1e-3, weight_decay=1e-3,\n",
    "                                  l1=0.0, l2=0.0, hidden_sizes=(512,256,128,64), patience=15, epochs=80)),\n",
    "    (\"InputNoise+L2\",        dict(dropout=0.00, batchnorm=False, spectral_norm=False,\n",
    "                                  input_noise_std=0.05, lr=1e-3, weight_decay=1e-3,\n",
    "                                  l1=0.0, l2=0.0, hidden_sizes=(512,256,128,64), patience=15, epochs=80)),\n",
    "]\n",
    "\n",
    "#Select the best config by validation DMSE, then retrain on full data\n",
    "best_name, best_val = None, float(\"inf\")\n",
    "best_cfg = None\n",
    "for name, cfg in configs:\n",
    "    model, _hist, val_dmse, _diags = train_one_model(\n",
    "        X_df2_stand_np, y_df2_np,\n",
    "        ticker_ids_train=ticker_ids_df2,\n",
    "        over_penalty=loss_penalty,\n",
    "        **cfg\n",
    "    )\n",
    "    if val_dmse < best_val:\n",
    "        best_val = val_dmse\n",
    "        best_name = name\n",
    "        best_cfg = cfg\n",
    "\n",
    "#Retrain best config on the full df2 window (no hold-out)\n",
    "best_backtest_nn_model, _hist_full, _score_out, _diags_full = train_one_model(\n",
    "    X_df2_stand_np, y_df2_np,\n",
    "    ticker_ids_train=ticker_ids_df2,\n",
    "    over_penalty=loss_penalty,\n",
    "    val_frac=0.0,  #no validation hold-out\n",
    "    **best_cfg\n",
    ")\n",
    "\n",
    "#Predict on the standardised backtest matrix\n",
    "with torch.no_grad():\n",
    "    xb = torch.as_tensor(X_backtest_stand_np, dtype=torch.float32, device=device)\n",
    "    preds_bt = best_backtest_nn_model(xb)\n",
    "    preds_bt = torch.clamp(preds_bt, min=0.0)\n",
    "    preds_bt_np = preds_bt.squeeze(-1).detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "#Attach predictions to df_backtest, keyed by Date+Ticker\n",
    "_pred_frame = df_backtest.loc[:, [\"Date\", \"Ticker\"]].copy()\n",
    "_pred_frame[\"Predictions_nn\"] = preds_bt_np\n",
    "\n",
    "df_backtest = (\n",
    "    df_backtest.drop(columns=[\"Predictions_nn\"], errors=\"ignore\")\n",
    "               .merge(_pred_frame, on=[\"Date\", \"Ticker\"], how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04c239",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35211a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the global random forest (non-standardised features)\n",
    "rf_backtest_model = RandomForestRegressor(\n",
    "    n_estimators=100, random_state=1, n_jobs=-1\n",
    ")\n",
    "rf_backtest_model.fit(X_df2_np, y_df2_np)\n",
    "\n",
    "#Predict on the backtest design matrix\n",
    "rf_backtest_pred = rf_backtest_model.predict(X_backtest_np).astype(np.float32)\n",
    "rf_backtest_pred = np.asarray(rf_backtest_pred).reshape(-1)  #ensure 1D for column assignment\n",
    "\n",
    "#Attach predictions to df_backtest, keyed by Date+Ticker\n",
    "_pred_frame = df_backtest.loc[:, [\"Date\", \"Ticker\"]].copy()\n",
    "_pred_frame[\"Predictions_rf\"] = rf_backtest_pred.astype(\"float32\")\n",
    "\n",
    "df_backtest = (\n",
    "    df_backtest.drop(columns=[\"Predictions_rf\"], errors=\"ignore\")\n",
    "               .merge(_pred_frame, on=[\"Date\", \"Ticker\"], how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc3fb88",
   "metadata": {},
   "source": [
    "##### ARIMA/ARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c542e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build per-row-aligned train and backtest frames from the arrays\n",
    "df_tr_arima = df2_stand.loc[:, [\"Date\", \"Ticker\"]].copy()\n",
    "df_tr_arima = pd.concat(\n",
    "    [df_tr_arima, pd.DataFrame(X_df2_stand_np, columns=predictors, index=df_tr_arima.index)],\n",
    "    axis=1\n",
    ")\n",
    "df_tr_arima[\"Target\"] = np.asarray(y_df2_np, dtype=float).ravel()\n",
    "\n",
    "df_bt_arima = df_backtest_stand.loc[:, [\"Date\", \"Ticker\"]].copy()\n",
    "df_bt_arima = pd.concat(\n",
    "    [df_bt_arima, pd.DataFrame(X_backtest_stand_np, columns=predictors, index=df_bt_arima.index)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#Per-ticker ARIMA/ARIMAX using the 4-step selection on reduced-frequency subseries,\n",
    "#but forecasting on the backtest window (no access to future y).\n",
    "def _predict_one_ticker_forecast(df_tr_tkr, df_bt_tkr, predictors, H=10, alpha=9.0, val_frac=0.10):\n",
    "    df_tr_tkr = df_tr_tkr.copy()\n",
    "    df_bt_tkr = df_bt_tkr.copy()\n",
    "    df_tr_tkr[\"Date\"] = pd.to_datetime(df_tr_tkr[\"Date\"])\n",
    "    df_bt_tkr[\"Date\"] = pd.to_datetime(df_bt_tkr[\"Date\"])\n",
    "\n",
    "    if df_bt_tkr.empty:\n",
    "        return np.array([], dtype=float), pd.Series([], dtype=\"datetime64[ns]\")\n",
    "\n",
    "    #Raw time index across train+backtest\n",
    "    df_all = (pd.concat([df_tr_tkr, df_bt_tkr], axis=0, ignore_index=True)\n",
    "                .sort_values(\"Date\").reset_index(drop=True))\n",
    "    df_all[\"t_raw\"] = np.arange(len(df_all), dtype=int)\n",
    "\n",
    "    df_tr = df_all.iloc[:len(df_tr_tkr)].copy()\n",
    "    df_bt = df_all.iloc[len(df_tr_tkr):].copy()\n",
    "\n",
    "    y_tr_all = df_tr[\"Target\"].to_numpy(dtype=float)\n",
    "    X_tr_all = df_tr[predictors].to_numpy(dtype=float) if predictors else None\n",
    "    X_bt_all = df_bt[predictors].to_numpy(dtype=float) if predictors else None\n",
    "    t_tr = df_tr[\"t_raw\"].to_numpy()\n",
    "    t_bt = df_bt[\"t_raw\"].to_numpy()\n",
    "\n",
    "    yhat_bt = np.full(shape=(len(df_bt),), fill_value=np.nan, dtype=float)\n",
    "\n",
    "    for r in range(H):\n",
    "        #Training subseries for remainder class r\n",
    "        idx_tr_r = np.flatnonzero(t_tr % H == r)\n",
    "        if idx_tr_r.size == 0:\n",
    "            continue\n",
    "        y_tr_r = y_tr_all[idx_tr_r]\n",
    "        X_tr_r = None if X_tr_all is None else X_tr_all[idx_tr_r, :]\n",
    "\n",
    "        #Initial order via BIC on full training subseries\n",
    "        p0, d0, q0 = _initial_order_bic(y_tr_r, None, alpha=alpha)\n",
    "\n",
    "        #Local grid\n",
    "        candidates = _local_grid_around(p0, d0, q0)\n",
    "\n",
    "        #Internal validation on the subseries (last 10%)\n",
    "        n = len(y_tr_r)\n",
    "        n_va = max(1, int(np.floor(val_frac * n)))\n",
    "        n_tr_sub = max(1, n - n_va)\n",
    "        y_tr_sub, y_va = y_tr_r[:n_tr_sub], y_tr_r[n_tr_sub:]\n",
    "        X_tr_sub = None if X_tr_r is None else X_tr_r[:n_tr_sub, :]\n",
    "        X_va     = None if X_tr_r is None else X_tr_r[n_tr_sub:, :]\n",
    "\n",
    "        best_tuple = None  #(mse, order, use_exog)\n",
    "        for od in candidates:\n",
    "            #ARIMA\n",
    "            yhat_a, _ = _fit_and_forecast(y_tr_sub, None, y_va, None, od, use_exog=False)\n",
    "            if yhat_a is not None:\n",
    "                mse_a = mse_np(y_va, yhat_a)\n",
    "                if (best_tuple is None) or (mse_a < best_tuple[0] - 1e-10) or \\\n",
    "                   (abs(mse_a - best_tuple[0]) <= 1e-10 and best_tuple[2] is True):\n",
    "                    best_tuple = (mse_a, od, False)\n",
    "            #ARIMAX\n",
    "            if X_tr_sub is not None:\n",
    "                yhat_x, _ = _fit_and_forecast(y_tr_sub, X_tr_sub, y_va, X_va, od, use_exog=True)\n",
    "                if yhat_x is not None:\n",
    "                    mse_x = mse_np(y_va, yhat_x)\n",
    "                    if (best_tuple is None) or (mse_x < best_tuple[0] - 1e-10) or \\\n",
    "                       (abs(mse_x - best_tuple[0]) <= 1e-10 and best_tuple[2] is False):\n",
    "                        best_tuple = (mse_x, od, True)\n",
    "\n",
    "        if best_tuple is None:\n",
    "            #Fallback to (0,0,0) ARIMA on subseries mean behaviour\n",
    "            best_tuple = (np.inf, (0, 0, 0), False)\n",
    "\n",
    "        _, best_order, best_use_exog = best_tuple\n",
    "\n",
    "        #Final refit on full training subseries and out-of-sample forecast for this remainder\n",
    "        idx_bt_r = np.flatnonzero(t_bt % H == r)\n",
    "        if idx_bt_r.size == 0:\n",
    "            continue\n",
    "\n",
    "        X_bt_r = None if X_bt_all is None else X_bt_all[idx_bt_r, :]\n",
    "\n",
    "        try:\n",
    "            res_full = _fit_sarimax(y_tr_r, (X_tr_r if best_use_exog else None), best_order)\n",
    "            if best_use_exog:\n",
    "                fc = res_full.get_forecast(steps=len(idx_bt_r), exog=X_bt_r)\n",
    "            else:\n",
    "                fc = res_full.get_forecast(steps=len(idx_bt_r))\n",
    "            yhat_r = np.asarray(fc.predicted_mean, dtype=float)\n",
    "        except Exception:\n",
    "            #Robust fallback: use the mean of the training subseries\n",
    "            yhat_r = np.full(shape=len(idx_bt_r), fill_value=float(np.mean(y_tr_r)), dtype=float)\n",
    "\n",
    "        yhat_bt[idx_bt_r] = yhat_r\n",
    "\n",
    "    #If any NaNs remain (e.g., empty subseries), fill with the overall training mean\n",
    "    if np.isnan(yhat_bt).any():\n",
    "        fill_val = float(np.nanmean(yhat_bt)) if np.isfinite(np.nanmean(yhat_bt)) else float(np.nanmean(y_tr_all))\n",
    "        yhat_bt = np.where(np.isnan(yhat_bt), fill_val, yhat_bt)\n",
    "\n",
    "    return yhat_bt, df_bt[\"Date\"].reset_index(drop=True)\n",
    "\n",
    "#Run per-ticker and collect predictions\n",
    "tickers_bt = sorted(set(df_tr_arima[\"Ticker\"]).intersection(set(df_bt_arima[\"Ticker\"])))\n",
    "pred_rows = []\n",
    "for tkr in tickers_bt:\n",
    "    tr_tkr = df_tr_arima.loc[df_tr_arima[\"Ticker\"] == tkr].sort_values(\"Date\").reset_index(drop=True)\n",
    "    bt_tkr = df_bt_arima.loc[df_bt_arima[\"Ticker\"] == tkr].sort_values(\"Date\").reset_index(drop=True)\n",
    "    if tr_tkr.empty or bt_tkr.empty:\n",
    "        continue\n",
    "    yhat_bt, bt_dates = _predict_one_ticker_forecast(tr_tkr, bt_tkr, predictors, H=H, alpha=9.0, val_frac=0.10)\n",
    "    pred_rows.append(pd.DataFrame({\n",
    "        \"Ticker\": tkr,\n",
    "        \"Date\":   bt_dates.to_numpy(),\n",
    "        \"Predictions_arima\": yhat_bt.astype(np.float32)\n",
    "    }))\n",
    "\n",
    "#Merge predictions into df_backtest by (Date, Ticker)\n",
    "if pred_rows:\n",
    "    preds_df_arima = pd.concat(pred_rows, axis=0, ignore_index=True)\n",
    "    \n",
    "    if pd.api.types.is_datetime64_any_dtype(df_backtest[\"Date\"]):\n",
    "        preds_df_arima[\"Date\"] = pd.to_datetime(preds_df_arima[\"Date\"])\n",
    "    else:\n",
    "        preds_df_arima[\"Date\"] = pd.to_datetime(preds_df_arima[\"Date\"]).dt.date\n",
    "\n",
    "    df_backtest = (\n",
    "        df_backtest.drop(columns=[\"Predictions_arima\"], errors=\"ignore\")\n",
    "                   .merge(preds_df_arima, on=[\"Date\", \"Ticker\"], how=\"left\")\n",
    "    )\n",
    "else:\n",
    "    df_backtest[\"Predictions_arima\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e84a4a",
   "metadata": {},
   "source": [
    "##### Long Short-Term Memory Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEQ_LEN   = 20\n",
    "HIDDEN    = 64\n",
    "LAYERS    = 2\n",
    "BIDIR     = True\n",
    "DROPOUT   = 0.20\n",
    "HEAD_H    = 128\n",
    "BATCH_T   = 64\n",
    "LR        = 3e-3\n",
    "EPOCHS    = 40\n",
    "PATIENCE  = 10\n",
    "ALPHA     = loss_penalty\n",
    "\n",
    "F = len(predictors)\n",
    "\n",
    "#Sequence builders\n",
    "def _sorted_group_indices(df):\n",
    "    dft = df.loc[:, [\"Date\", \"Ticker\"]].copy()\n",
    "    dft[\"Date\"] = pd.to_datetime(dft[\"Date\"])\n",
    "    pos = pd.Series(np.arange(len(dft), dtype=int), index=dft.index)  \n",
    "    idxs = {}\n",
    "    for tkr, g in dft.groupby(\"Ticker\", sort=False, as_index=False):\n",
    "        g_sorted = g.sort_values(\"Date\")\n",
    "        idxs[tkr] = pos.loc[g_sorted.index].to_numpy()  \n",
    "    return idxs\n",
    "\n",
    "def make_sequences_from_panel(df_ref, X2d, y2d_or_none, seq_len):\n",
    "    \"\"\"\n",
    "    Build rolling (seq_len)-step sequences per ticker from flat 2D arrays aligned to df_ref rows.\n",
    "    Returns:\n",
    "      X_seq : (N_samples, seq_len, F)\n",
    "      y_seq : (N_samples, 1) or None\n",
    "      meta  : DataFrame with columns [\"Date\",\"Ticker\"] for the sequence *end* row\n",
    "    \"\"\"\n",
    "    tkr_to_idxs = _sorted_group_indices(df_ref)\n",
    "    X_seq_list, y_seq_list, meta_rows = [], [], []\n",
    "\n",
    "    for tkr, idx in tkr_to_idxs.items():\n",
    "        if idx.size <= seq_len:\n",
    "            continue\n",
    "        #rolling windows that *end* at position j (seq covers rows j-seq_len ... j-1)\n",
    "        for j in range(seq_len, idx.size):\n",
    "            win = idx[j-seq_len:j]\n",
    "            end_row = idx[j]\n",
    "            X_seq_list.append(X2d[win, :])\n",
    "            if y2d_or_none is not None:\n",
    "                y_seq_list.append(y2d_or_none[end_row])\n",
    "            meta_rows.append((df_ref.iloc[end_row][\"Date\"], df_ref.iloc[end_row][\"Ticker\"]))\n",
    "\n",
    "    X_seq = np.stack(X_seq_list, axis=0).astype(\"float32\") if X_seq_list else np.empty((0, seq_len, F), dtype=\"float32\")\n",
    "    y_seq = (np.vstack(y_seq_list).astype(\"float32\") if y_seq_list else None)\n",
    "    meta = pd.DataFrame(meta_rows, columns=[\"Date\",\"Ticker\"])\n",
    "    return X_seq, y_seq, meta\n",
    "\n",
    "def make_backtest_sequences_with_history(df_hist, X_hist, df_bt, X_bt, seq_len):\n",
    "    \"\"\"\n",
    "    For each ticker, take the last `seq_len` rows from df_hist, then all rows from df_bt.\n",
    "    Build one sequence per backtest row whose window ends at that backtest date.\n",
    "    Returns X_seq (N_bt, seq_len, F) and meta (Date, Ticker) for the backtest rows.\n",
    "    \"\"\"\n",
    "    dfh = df_hist.loc[:, [\"Date\",\"Ticker\"]].copy(); dfh[\"Date\"] = pd.to_datetime(dfh[\"Date\"])\n",
    "    dfb = df_bt.loc[:, [\"Date\",\"Ticker\"]].copy();  dfb[\"Date\"] = pd.to_datetime(dfb[\"Date\"])\n",
    "\n",
    "    pos_h = pd.Series(np.arange(len(dfh), dtype=int), index=dfh.index)   #label -> position\n",
    "    pos_b = pd.Series(np.arange(len(dfb), dtype=int), index=dfb.index)\n",
    "\n",
    "    X_seq_list, meta_rows = [], []\n",
    "    F = X_hist.shape[1]\n",
    "\n",
    "    for tkr, g_b in dfb.groupby(\"Ticker\", sort=False):\n",
    "        g_b = g_b.sort_values(\"Date\")\n",
    "        g_h = dfh[dfh[\"Ticker\"] == tkr].sort_values(\"Date\")\n",
    "        if g_b.empty or g_h.empty:\n",
    "            continue\n",
    "\n",
    "        h_pos = pos_h.loc[g_h.index].to_numpy()\n",
    "        b_pos = pos_b.loc[g_b.index].to_numpy()\n",
    "\n",
    "        #last seq_len rows from history (pad by repeating earliest if needed)\n",
    "        if h_pos.size < seq_len:\n",
    "            pad = seq_len - h_pos.size\n",
    "            h_tail = np.pad(h_pos, (pad, 0), mode=\"edge\")\n",
    "        else:\n",
    "            h_tail = h_pos[-seq_len:]\n",
    "\n",
    "        #extended feature block: [history tail | entire backtest]\n",
    "        X_ext = np.vstack([X_hist[h_tail, :], X_bt[b_pos, :]])\n",
    "        T_hist = seq_len  #length of the history tail inside X_ext\n",
    "\n",
    "        #one sequence per backtest date; window ends at each backtest row\n",
    "        for k in range(len(b_pos)):\n",
    "            end = T_hist + k         #exclusive\n",
    "            start = end - seq_len\n",
    "            X_seq_list.append(X_ext[start:end, :])\n",
    "            meta_rows.append((g_b[\"Date\"].iloc[k], tkr))\n",
    "\n",
    "    X_seq = np.stack(X_seq_list, axis=0).astype(\"float32\") if X_seq_list else np.empty((0, seq_len, F), dtype=\"float32\")\n",
    "    meta = pd.DataFrame(meta_rows, columns=[\"Date\",\"Ticker\"])\n",
    "    return X_seq, meta\n",
    "\n",
    "\n",
    "#Build train sequences from df2 (uses y) and backtest sequences from df_backtest (no y)\n",
    "X_train_seq, y_train_seq, meta_train = make_sequences_from_panel(df2, X_df2_np, y_df2_np, SEQ_LEN)\n",
    "X_bt_seq, meta_bt = make_backtest_sequences_with_history(df2, X_df2_np, df_backtest, X_backtest_np, SEQ_LEN)\n",
    "\n",
    "#Chronological split (per ticker)\n",
    "def chrono_val_split_indices(meta_df, val_frac=0.10):\n",
    "    \"\"\"\n",
    "    Given meta_df with columns [\"Date\",\"Ticker\"] (one row per sequence end),\n",
    "    return (train_idx, val_idx) indices implementing a per-ticker chronological split\n",
    "    where the last val_frac sequences for each ticker go to validation.\n",
    "    \"\"\"\n",
    "    meta = meta_df.copy()\n",
    "    meta[\"Date\"] = pd.to_datetime(meta[\"Date\"])\n",
    "    tr_idx, va_idx = [], []\n",
    "    for tkr, g in meta.groupby(\"Ticker\", sort=False):\n",
    "        g_sorted = g.sort_values(\"Date\")\n",
    "        n = len(g_sorted)\n",
    "        k = max(1, int(np.floor(n * (1.0 - val_frac))))\n",
    "        idx_sorted = g_sorted.index.to_numpy()\n",
    "        tr_idx.append(idx_sorted[:k])\n",
    "        va_idx.append(idx_sorted[k:])\n",
    "    tr_idx = np.concatenate(tr_idx) if tr_idx else np.array([], dtype=int)\n",
    "    va_idx = np.concatenate(va_idx) if va_idx else np.array([], dtype=int)\n",
    "    return tr_idx, va_idx\n",
    "\n",
    "tr_idx, va_idx = chrono_val_split_indices(meta_train, val_frac=0.10)\n",
    "\n",
    "#Tensor datasets/loaders\n",
    "Xtr_np, ytr_np = X_train_seq[tr_idx], y_train_seq[tr_idx]\n",
    "Xva_np, yva_np = X_train_seq[va_idx], y_train_seq[va_idx]\n",
    "\n",
    "train_dl = DataLoader(TensorDataset(torch.from_numpy(Xtr_np), torch.from_numpy(ytr_np)),\n",
    "                      batch_size=BATCH_T, shuffle=True, drop_last=False)\n",
    "val_dl   = DataLoader(TensorDataset(torch.from_numpy(Xva_np), torch.from_numpy(yva_np)),\n",
    "                      batch_size=BATCH_T, shuffle=False, drop_last=False)\n",
    "\n",
    "#Model\n",
    "class LSTMRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    BiLSTM over the SEQ_LEN window with a non-negative Softplus head.\n",
    "    Input:  (B, SEQ_LEN, F)\n",
    "    Output: (B,)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size=HIDDEN, num_layers=LAYERS,\n",
    "                 bidirectional=BIDIR, dropout=DROPOUT, head_hidden=HEAD_H):\n",
    "        super().__init__()\n",
    "        self.D = 2 if bidirectional else 1\n",
    "        self.in_norm = nn.LayerNorm(input_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=(dropout if num_layers > 1 else 0.0),\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * self.D, head_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.10),\n",
    "            nn.Linear(head_hidden, 1),\n",
    "            nn.Softplus()     #keep predictions >= 0\n",
    "        )\n",
    "\n",
    "    def forward(self, x):               #x: (B, SEQ_LEN, F)\n",
    "        x = self.in_norm(x)\n",
    "        out, _ = self.lstm(x)           #(B, SEQ_LEN, H*D)\n",
    "        h_last = out[:, -1, :]          #last timestep summary\n",
    "        return self.head(h_last).squeeze(-1)\n",
    "\n",
    "model = LSTMRegressor(input_size=F).to(device)\n",
    "criterion = DMSE(over_penalty=ALPHA).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=2, min_lr=1e-5, verbose=False\n",
    ")\n",
    "\n",
    "def eval_dmse(loader):\n",
    "    model.eval()\n",
    "    tot_loss, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            yhat = model(xb).unsqueeze(-1)\n",
    "            loss = criterion(y_hat=yhat, y=yb)\n",
    "            tot_loss += float(loss.item()) * yb.size(0)\n",
    "            n += yb.size(0)\n",
    "    return tot_loss / max(1, n)\n",
    "\n",
    "#Train\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        yhat = model(xb).unsqueeze(-1)\n",
    "        loss = criterion(y_hat=yhat, y=yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    #Validate\n",
    "    val_loss = eval_dmse(val_dl) if len(Xva_np) > 0 else eval_dmse(train_dl)\n",
    "    scheduler.step(val_loss)\n",
    "    if val_loss + 1e-10 < best_val:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "#Predict on backtest sequences\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    xb = torch.from_numpy(X_bt_seq).to(device)\n",
    "    preds_seq = model(xb).clamp_min(0.0)           \n",
    "    preds_seq_np = preds_seq.detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "#Attach to df_backtest\n",
    "preds_df = meta_bt.copy()\n",
    "if pd.api.types.is_datetime64_any_dtype(df_backtest[\"Date\"]):\n",
    "    preds_df[\"Date\"] = pd.to_datetime(preds_df[\"Date\"])\n",
    "else:\n",
    "    preds_df[\"Date\"] = pd.to_datetime(preds_df[\"Date\"]).dt.date\n",
    "\n",
    "preds_df[\"Predictions_lstmnn\"] = preds_seq_np\n",
    "\n",
    "df_backtest = (\n",
    "    df_backtest.drop(columns=[\"Predictions_lstmnn\"], errors=\"ignore\")\n",
    "               .merge(preds_df, on=[\"Date\",\"Ticker\"], how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff153319",
   "metadata": {},
   "source": [
    "### Trading Strategy Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f2a79f",
   "metadata": {},
   "source": [
    "##### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the predictions obtained above, we compute the associated implied expected profits from 10-day ATM straddles\n",
    "models_names = [\"lr\", \"nn\", \"rf\", \"arima\", \"lstmnn\"]\n",
    "for model in models_names:\n",
    "    df_backtest[f\"EP_{model}\"] = df_backtest[f\"Predictions_{model}\"] * df_backtest[\"Close\"] - df_backtest[\"Straddle_premium\"]\n",
    "\n",
    "#We eliminate the columns we do not need any more\n",
    "df_backtest.drop(columns = predictors + [\"Sector\", \"Volume\", \"Slope\"] + [f\"Predictions_{model}\" for model in models_names],\n",
    "                 axis=1, inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4751084",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Position:\n",
    "    ticker: str\n",
    "    entry_date: pd.Timestamp\n",
    "    expiry_deadline: pd.Timestamp   \n",
    "    strike: float                 \n",
    "    premium_per_share: float     \n",
    "    ep_per_share: float        \n",
    "    tp_intrinsic_per_share: float \n",
    "    n_contracts: int    \n",
    "    gross_cost: float    \n",
    "    total_cost_out: float       \n",
    "    open: bool = True\n",
    "\n",
    "\n",
    "def backtest_strategy(\n",
    "    input_df: pd.DataFrame,\n",
    "    *,\n",
    "    model: str,                                #\"lr\", \"nn\", \"rf\", \"arima\", or \"lstmnn\"\n",
    "    capital: float,\n",
    "    capital_fraction: Optional[np.ndarray] = None,\n",
    "    commissions: float = 0.01,      \n",
    "    threshold: float = 0.10,  \n",
    "    do_plot: bool = False,       \n",
    "    plot_number: int = 1,\n",
    "    seed: int = 0\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Backtests an ATM straddle strategy with exercise-only exits.\n",
    "\n",
    "    Key rules implemented:\n",
    "    - Per-ticker capital buckets (fixed at start via capital_fraction; equal weights if None).\n",
    "    - Entry (right before the close): buy an ATM straddle iff EP_per_share >= threshold * premium_per_share,\n",
    "      apply commissions on buy.\n",
    "    - Expiry: at the first trading day with date >= entry_date + 10 calendar days we exercise at close intrinsic.\n",
    "    - Portfolio value is marked daily\n",
    "    - Option contract multiplier = 100.\n",
    "    - One new entry per ticker per day at most.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    df = input_df.copy()\n",
    "    tickers = df[\"Ticker\"].drop_duplicates().tolist()\n",
    "    n_tickers = len(tickers)\n",
    "    model = model.lower()\n",
    "    ep_col = f\"EP_{model}\"\n",
    "\n",
    "    #Build fast per-ticker frames (Date as index)\n",
    "    per_ticker: Dict[str, pd.DataFrame] = {\n",
    "        t: g.set_index(\"Date\")[[\"High\", \"Low\", \"Close\", \"Straddle_premium\", ep_col]].sort_index()\n",
    "        for t, g in df.groupby(\"Ticker\", sort=False)\n",
    "    }\n",
    "\n",
    "    #Use the shared calendar of dates \n",
    "    all_dates = df[\"Date\"].drop_duplicates().sort_values().tolist()\n",
    "    first_date, last_date = all_dates[0], all_dates[-1]\n",
    "\n",
    "    #Capital buckets\n",
    "    if capital_fraction is None:\n",
    "        weights = np.full(n_tickers, 1.0 / n_tickers, dtype=float)\n",
    "    else:\n",
    "        cf = np.asarray(capital_fraction, dtype=float)\n",
    "        if cf.shape[0] != n_tickers:\n",
    "            raise ValueError(\"capital_fraction length must equal the number of tickers in df_backtest.\")\n",
    "        total = cf.sum()\n",
    "        if total <= 0:\n",
    "            raise ValueError(\"capital_fraction must sum to a positive value.\")\n",
    "        weights = cf / total\n",
    "\n",
    "    starting_cash: Dict[str, float] = {t: float(capital * w) for t, w in zip(tickers, weights)}\n",
    "    free_cash: Dict[str, float] = starting_cash.copy()\n",
    "\n",
    "    #Positions and trade log\n",
    "    open_positions: Dict[str, List[Position]] = {t: [] for t in tickers}\n",
    "    closed_trades: List[dict] = []\n",
    "\n",
    "    def _get_row(t: str, d: pd.Timestamp):\n",
    "        row = per_ticker[t].loc[d]\n",
    "        #row is a Series with indices: High, Low, Close, Straddle_premium, ep_col\n",
    "        return float(row[\"High\"]), float(row[\"Low\"]), float(row[\"Close\"]), float(row[\"Straddle_premium\"]), float(row[ep_col])\n",
    "\n",
    "    #Main backtest loop over dates\n",
    "    equity_dates = []\n",
    "    equity_values = []\n",
    "\n",
    "    for current_date in all_dates:\n",
    "        #Handle exits first \n",
    "        for t in tickers:\n",
    "            if current_date not in per_ticker[t].index:\n",
    "                continue\n",
    "            high, low, close, _, _ = _get_row(t, current_date)\n",
    "\n",
    "            #iterate over a copy to allow removal\n",
    "            remaining_positions: List[Position] = []\n",
    "            for pos in open_positions[t]:\n",
    "                if not pos.open:\n",
    "                    continue\n",
    "\n",
    "                exercised = False\n",
    "                payout = 0.0\n",
    "\n",
    "                #If still alive in calendar-time\n",
    "                if current_date <= pos.expiry_deadline:\n",
    "                    max_intraday_intrinsic = max(max(high - pos.strike, 0.0),\n",
    "                                                 max(pos.strike - low, 0.0))\n",
    "                    if max_intraday_intrinsic >= pos.tp_intrinsic_per_share:\n",
    "                        realized_per_share = pos.tp_intrinsic_per_share\n",
    "                        payout = realized_per_share * 100.0 * pos.n_contracts\n",
    "                        cash_in = max(payout - 1 * pos.n_contracts, 0.0) * (1.0 - commissions)\n",
    "                        free_cash[t] += cash_in\n",
    "                        exercised = True\n",
    "\n",
    "                        pnl = cash_in - pos.total_cost_out\n",
    "                        closed_trades.append({\n",
    "                            \"Ticker\": t,\n",
    "                            \"EntryDate\": pos.entry_date,\n",
    "                            \"ExitDate\": current_date,\n",
    "                            \"DaysHeld\": (current_date - pos.entry_date).days,\n",
    "                            \"Strike\": pos.strike,\n",
    "                            \"Premium_per_share\": pos.premium_per_share,\n",
    "                            \"Contracts\": pos.n_contracts,\n",
    "                            \"ExitReason\": \"TakeProfit\",\n",
    "                            \"GrossPayout\": payout,\n",
    "                            \"PnL\": pnl\n",
    "                        })\n",
    "\n",
    "                #If not already exercised, check expiration\n",
    "                if (not exercised) and (current_date >= pos.expiry_deadline):\n",
    "                    intrinsic_at_close = max(abs(close - pos.strike), 0.0)\n",
    "                    payout = intrinsic_at_close * 100.0 * pos.n_contracts\n",
    "                    cash_in = max(payout - 1*pos.n_contracts, 0.0) * (1.0 - commissions)\n",
    "                    free_cash[t] += cash_in\n",
    "                    exercised = True\n",
    "\n",
    "                    pnl = cash_in - pos.total_cost_out\n",
    "                    closed_trades.append({\n",
    "                        \"Ticker\": t,\n",
    "                        \"EntryDate\": pos.entry_date,\n",
    "                        \"ExitDate\": current_date,\n",
    "                        \"DaysHeld\": (current_date - pos.entry_date).days,\n",
    "                        \"Strike\": pos.strike,\n",
    "                        \"Premium_per_share\": pos.premium_per_share,\n",
    "                        \"Contracts\": pos.n_contracts,\n",
    "                        \"ExitReason\": \"Expiry\",\n",
    "                        \"GrossPayout\": payout,\n",
    "                        \"PnL\": pnl\n",
    "                    })\n",
    "\n",
    "                if exercised:\n",
    "                    pos.open = False\n",
    "                else:\n",
    "                    remaining_positions.append(pos)\n",
    "\n",
    "            open_positions[t] = remaining_positions\n",
    "\n",
    "        for t in tickers:\n",
    "            if current_date not in per_ticker[t].index:\n",
    "                continue\n",
    "            high, low, close, premium_ps, _ep_today = _get_row(t, current_date)\n",
    "            ep_ps = float(per_ticker[t].loc[current_date, ep_col])\n",
    "            if np.isnan(ep_ps) or np.isnan(premium_ps):\n",
    "                continue\n",
    "\n",
    "            #Entry condition: EP >= threshold * premium\n",
    "            if ep_ps >= threshold * premium_ps:\n",
    "                #Budget exactly 10% of the ticker's current free cash for this trade\n",
    "                allow_cash = 0.1 * free_cash[t]\n",
    "                if allow_cash <= 0:\n",
    "                    continue\n",
    "\n",
    "                cost_per_contract_out = premium_ps * 100.0 * (1.0 + commissions)\n",
    "                n_contracts = int(allow_cash // cost_per_contract_out)\n",
    "                if n_contracts < 1:\n",
    "                    continue\n",
    "                \n",
    "                tp_intrinsic_ps = ((1.08 * premium_ps * 100.0 * (1.0 + commissions)) / (1.0 - commissions) + 1.0) / 100.0\n",
    "                gross_cost = premium_ps * 100.0 * n_contracts\n",
    "                total_cost_out = gross_cost * (1.0 + commissions)\n",
    "                free_cash[t] -= total_cost_out\n",
    "\n",
    "                pos = Position(\n",
    "                    ticker=t,\n",
    "                    entry_date=current_date,\n",
    "                    expiry_deadline=current_date + pd.Timedelta(days=10),\n",
    "                    strike=close,\n",
    "                    premium_per_share=premium_ps,\n",
    "                    ep_per_share=ep_ps,\n",
    "                    tp_intrinsic_per_share=tp_intrinsic_ps,\n",
    "                    n_contracts=n_contracts,\n",
    "                    gross_cost=gross_cost,\n",
    "                    total_cost_out=total_cost_out,\n",
    "                    open=True\n",
    "                )\n",
    "                open_positions[t].append(pos)\n",
    "\n",
    "        equity = sum(free_cash.values())\n",
    "        for t in tickers:\n",
    "            if current_date not in per_ticker[t].index:\n",
    "                continue\n",
    "            _, _, close, _, _ = _get_row(t, current_date)\n",
    "            for pos in open_positions[t]:\n",
    "                if pos.open:\n",
    "                    intrinsic = max(abs(close - pos.strike), 0.0)\n",
    "                    equity += intrinsic * 100.0 * pos.n_contracts\n",
    "\n",
    "        equity_dates.append(current_date)\n",
    "        equity_values.append(equity)\n",
    "\n",
    "    #Results assembly\n",
    "    equity_df = pd.DataFrame({\"Date\": equity_dates, \"Equity\": equity_values})\n",
    "    equity_df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "    trades_df = pd.DataFrame(closed_trades)\n",
    "    if not trades_df.empty:\n",
    "        trades_df.sort_values([\"ExitDate\", \"Ticker\", \"EntryDate\"], inplace=True)\n",
    "\n",
    "    #Statistics\n",
    "    final_equity = float(equity_df[\"Equity\"].iloc[-1])\n",
    "    total_pnl_pct = (final_equity - capital) / capital\n",
    "\n",
    "    #Daily log returns and risk metrics\n",
    "    eq = equity_df[\"Equity\"].astype(float)\n",
    "    rets = np.log(eq/eq.shift(1)).dropna()\n",
    "\n",
    "    #Risk-free rate assumption for 2022-2024 period\n",
    "    annual_rf_rate = 0.04  #4% annual risk-free rate \n",
    "    daily_rf_rate = annual_rf_rate / 252  #Convert to daily rate\n",
    "    daily_rf_log = np.log(1 + daily_rf_rate)  #Convert to log-return equ\n",
    "\n",
    "    daily_vol = rets.std(ddof=1)           #std of daily log-returns\n",
    "    ann_vol = daily_vol * np.sqrt(252)     #annualised volatility\n",
    "\n",
    "    sharpe = np.nan\n",
    "    if daily_vol > 0:\n",
    "        #Subtract log risk-free rate from log-returns\n",
    "        excess_daily_log_return = rets.mean() - daily_rf_log\n",
    "        sharpe = (excess_daily_log_return / daily_vol) * np.sqrt(252)\n",
    "    \n",
    "    #CAGR\n",
    "    n_days = (equity_df.index[-1] - equity_df.index[0]).days\n",
    "    cagr = np.nan\n",
    "    if n_days > 0 and capital > 0:\n",
    "        years = n_days / 365.25\n",
    "        cagr = (final_equity / capital) ** (1 / years) - 1\n",
    "\n",
    "    #Max drawdown\n",
    "    roll_max = eq.cummax()\n",
    "    dd = eq / roll_max - 1.0\n",
    "    max_dd = dd.min()\n",
    "    max_dd = float(max_dd) if pd.notna(max_dd) else np.nan\n",
    "\n",
    "    #Trade stats\n",
    "    n_trades = int(trades_df.shape[0]) if not trades_df.empty else 0\n",
    "    gross_win = trades_df.loc[trades_df[\"PnL\"] > 0, \"PnL\"].sum() if n_trades > 0 else 0.0\n",
    "    gross_loss = -trades_df.loc[trades_df[\"PnL\"] < 0, \"PnL\"].sum() if n_trades > 0 else 0.0\n",
    "    profit_factor = (gross_win / gross_loss) if gross_loss > 0 else np.inf\n",
    "\n",
    "    expectancy = trades_df[\"PnL\"].mean() if n_trades > 0 else np.nan  #average PnL per trade\n",
    "\n",
    "    stats = {\n",
    "        \"Start Date\": str(first_date),\n",
    "        \"End Date\": str(last_date),\n",
    "        \"Initial Equity\": round(capital, 2),\n",
    "        \"Final Equity\": round(final_equity, 2),\n",
    "        \"PnL\": f\"{round(total_pnl_pct*100, 2)}%\",\n",
    "        \"Annualised Volatility\": f\"{round(100 * ann_vol, 2)}%\" if not np.isnan(ann_vol) else np.nan,\n",
    "        \"Sharpe Ratio\": round(sharpe, 3) if not np.isnan(sharpe) else np.nan,\n",
    "        \"CAGR\": f\"{round(100 * cagr, 2)}%\" if not np.isnan(cagr) else np.nan,\n",
    "        \"Maximum Drawdown\": f\"{round(100 * max_dd, 2)}%\" if not np.isnan(max_dd) else np.nan,\n",
    "        \"Profit Factor\": (round(profit_factor, 3) if np.isfinite(profit_factor) else \"inf\"),\n",
    "        \"Per-Trade Expectancy\": round(expectancy, 2) if not np.isnan(expectancy) else np.nan\n",
    "    }\n",
    "\n",
    "    if do_plot:\n",
    "        model_names_dict = {\"lr\":\"Linear Regression\", \"nn\":\"Feedforward Neural Network\", \"rf\":\"Random Forest\", \n",
    "                        \"arima\":\"ARIMA(X)\", \"lstmnn\":\"Long Short-Term Memory Neural Network\"}\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(equity_df.index, equity_df[\"Equity\"], color=\"green\")\n",
    "        plt.title(f\"Capital Curve – {model_names_dict[model]}\", fontsize=22, weight=\"bold\")\n",
    "        plt.xlabel(\"Date\", fontsize=25)\n",
    "        plt.ylabel(\"Total Capital\", fontsize=25)\n",
    "        plt.tick_params(axis=\"x\", colors=\"black\", labelsize=13)\n",
    "        plt.tick_params(axis=\"y\", colors=\"black\", labelsize=16)\n",
    "        plt.grid(True, which=\"major\", linestyle=\"-\", alpha=0.15)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    stats_df = pd.DataFrame(list(stats.items()), columns=[\"Metric\", \"Value\"]).iloc[2:].reset_index(drop=True)\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246bf476",
   "metadata": {},
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd69ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"lr\", \"nn\", \"rf\", \"arima\", \"lstmnn\"]\n",
    "l = [x/100 for x in range(5,201,5)]\n",
    "models_best_threshold = {\"lr\":0.05, \"nn\":0.05, \"rf\":0.05, \"arima\":0.05, \"lstmnn\":0.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4edc63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(f\"Model = {model_name}\")\n",
    "    best_sharpe = -np.inf\n",
    "    for x in l:\n",
    "        sharpe_ratio = backtest_strategy(input_df=df_backtest, model=model_name, capital=100000, \n",
    "                       capital_fraction=None, commissions=0.01, threshold=x, do_plot=False).iloc[4,1]\n",
    "        print(f\"threshold = {x}, sharpe = {sharpe_ratio}\")\n",
    "        if sharpe_ratio > best_sharpe:\n",
    "            models_best_threshold[model_name] = x\n",
    "            best_sharpe = sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b03fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_strategy(input_df=df_backtest, model=\"lr\", capital=100000, capital_fraction=None, \n",
    "                  commissions=0.01, threshold=models_best_threshold[\"lr\"], do_plot=True, plot_number=153)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c36ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_strategy(input_df=df_backtest, model=\"nn\", capital=100000, capital_fraction=None, \n",
    "                  commissions=0.01, threshold=models_best_threshold[\"nn\"], do_plot=True, plot_number=154)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2970b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_strategy(input_df=df_backtest, model=\"rf\", capital=100000, capital_fraction=None, \n",
    "                  commissions=0.01, threshold=models_best_threshold[\"rf\"], do_plot=True, plot_number=155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_strategy(input_df=df_backtest, model=\"arima\", capital=100000, capital_fraction=None, \n",
    "                  commissions=0.01, threshold=models_best_threshold[\"arima\"], do_plot=True, plot_number=156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753dd609",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_strategy(input_df=df_backtest, model=\"lstmnn\", capital=100000, capital_fraction=None, \n",
    "                  commissions=0.01, threshold=models_best_threshold[\"lstmnn\"], do_plot=True, plot_number=157)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
